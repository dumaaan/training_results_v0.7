+ echo 'Beginning trial 4 of 5'
Beginning trial 4 of 5
+ srun --ntasks=64 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592881311311, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592881311342, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592881311342, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592881311342, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592881311342, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "64xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=64 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n015
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n006
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n032
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n042
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n004
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n055
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n026
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n022
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n012
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n095
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n016
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n097
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n030
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n047
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n005
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n056
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n023
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n009
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n025
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n094
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n057
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n018
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n054
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n031
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n058
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n010
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n034
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n029
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n036
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n014
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n044
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n028
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n059
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n027
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n043
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n013
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n035
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n052
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n007
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n021
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n017
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n011
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n039
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n053
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n024
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n008
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n049
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n040
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n020
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n002
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n051
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n093
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n048
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n096
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n033
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n019
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n003
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n060
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n001
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=64 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592881318304, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318317, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318325, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318327, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318330, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318338, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318344, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318347, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318348, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318351, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318355, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318360, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318361, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318366, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318367, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318369, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318372, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318373, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318379, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318380, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318390, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318391, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318394, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318396, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318396, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318398, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318399, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318399, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318405, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318412, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318415, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318417, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318417, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318417, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318422, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318425, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318426, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318427, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318427, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318430, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318436, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318435, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318435, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318437, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318438, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318439, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318440, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318440, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318443, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318443, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318446, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318454, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318457, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318457, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318459, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318470, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318475, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318482, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318490, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318502, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318508, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318519, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318519, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881318523, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=1024 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/14043861/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' -n 6 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 5 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 2 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 3 ']'
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ declare -a CMD
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 3 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ '[' -n 6 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ '[' -n 10 ']'
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 15 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 11 ']'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ '[' -n 10 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 2 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 0 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 15 ']'
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ '[' -n 3 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DATASET_DIR=/data
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ '[' -n 5 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ echo 'running benchmark'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 7 ']'
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ echo 'running benchmark'
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 11 ']'
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 14 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 7 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ '[' -n 3 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ DATASET_DIR=/data
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
running benchmark
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 11 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
running benchmark
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ NUMEPOCHS=14
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ DATASET_DIR=/data
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ '[' -n 14 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ echo 'running benchmark'
+ '[' -n 6 ']'
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 11 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 9 ']'
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 10 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 1 ']'
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 4 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 6 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 15 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ LR=5.0e-3
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
running benchmark
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
running benchmark
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ MATH=fp16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
running benchmark
+ declare -a CMD
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 4 ']'
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 8 ']'
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ '[' -n 8 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ WARMUP_STEPS=200
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
running benchmark
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 6 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 1 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
running benchmark
+ TARGET=24.0
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' -n 6 ']'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 3 ']'
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ REMAIN_STEPS=1605
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
running benchmark
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MATH=fp16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 4 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
running benchmark
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 5 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
running benchmark
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
running benchmark
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 7 ']'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 14 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' -n 3 ']'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 3 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 1 ']'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 14 ']'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DECAY_INTERVAL=201
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 13 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TARGET=24.0
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
running benchmark
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ TARGET=24.0
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' -n 7 ']'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DIST_OPTS=
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 12 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 12 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 3 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 13 ']'
+ WARMUP_STEPS=200
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 7 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' -n 10 ']'
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 3 ']'
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DATASET_DIR=/data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 7 ']'
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
running benchmark
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' -n 15 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ NUMEPOCHS=14
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ DECAY_INTERVAL=201
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ declare -a CMD
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 2 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 1 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 7 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 11 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
running benchmark
+ '[' -n 8 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
running benchmark
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ LR=5.0e-3
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ '[' -n 12 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 8 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ DIST_OPTS=
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ '[' -n 12 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 3 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ '[' -n 6 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 8 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ MATH=fp16
+ LR=5.0e-3
+ TARGET=24.0
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 14 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
running benchmark
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 13 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
+ '[' -n 6 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 13 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 10 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ '[' -n 5 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 10 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 15 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 10 ']'
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ '[' -n 4 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ DATASET_DIR=/data
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ WARMUP_STEPS=200
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ '[' -n 6 ']'
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
running benchmark
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 10 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ '[' -n 0 ']'
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 7 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
running benchmark
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ echo 'running benchmark'
+ declare -a CMD
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ '[' -n 9 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
running benchmark
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ '[' -n 11 ']'
+ '[' -n 13 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 13 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 2 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 3 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ '[' -n 11 ']'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 10 ']'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 10 ']'
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 5 ']'
+ declare -a CMD
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ echo 'running benchmark'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 4 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 8 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 10 ']'
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 14 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 8 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ '[' -n 12 ']'
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 12 ']'
+ '[' -n 7 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 2 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 3 ']'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 7 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 5 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
running benchmark
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ '[' -n 11 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DIST_OPTS=
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' -n 13 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 13 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
running benchmark
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 08:02:01 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323214, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323268, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323268, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323269, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323269, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323272, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323271, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323310, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881323319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323326, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323339, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323343, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323348, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323359, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323364, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323365, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323368, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323368, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323374, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323380, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323381, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323385, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323391, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323393, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323403, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323403, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323404, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323415, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323416, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323416, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323427, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323431, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323436, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323441, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323443, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323443, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323451, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323460, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323459, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323462, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323464, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323465, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323468, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323475, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323477, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323481, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323483, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323489, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323495, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323494, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323495, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323501, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323508, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323507, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323517, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323521, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323526, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323528, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323534, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323537, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323547, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323549, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323554, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323564, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323575, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323575, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323576, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323578, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323583, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323585, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323586, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323593, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323600, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323601, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323607, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323623, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323695, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323716, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323861, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323889, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323902, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323918, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323925, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323930, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323930, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323937, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323945, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323945, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323952, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323953, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323955, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323955, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323957, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323957, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323963, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323962, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323965, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323963, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323968, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323973, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323977, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323977, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323977, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323979, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323981, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323983, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323982, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323983, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323983, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323984, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323986, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323986, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323988, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323988, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323988, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323988, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323989, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323990, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323993, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323996, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323997, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324000, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881323999, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324000, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324000, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324000, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324004, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324004, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324004, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324008, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324013, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324016, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324015, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324021, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324026, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324026, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324027, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324036, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324037, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324040, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324038, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324047, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324047, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324052, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324052, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324052, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324055, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324055, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324055, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324055, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324067, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324067, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324071, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324071, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324085, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324085, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324088, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324086, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324086, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324088, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324088, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324100, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324100, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324117, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324117, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324118, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324118, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324118, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324118, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324122, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324127, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324127, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324129, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324127, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324129, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324131, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324136, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324145, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324145, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324147, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324153, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324154, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324154, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324157, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324159, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324159, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324164, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324165, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324164, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324170, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324170, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324170, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324177, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324177, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324178, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324178, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324178, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324177, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324183, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324183, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324183, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324187, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324187, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324187, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324195, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324193, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324197, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324195, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324195, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324197, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324198, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324197, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324198, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324198, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324198, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324200, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324203, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324204, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324214, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324214, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324220, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324227, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324230, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324230, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324230, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324234, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324234, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324237, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324240, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324237, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324245, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324244, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324245, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324249, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324251, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324251, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324255, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324261, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324261, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324265, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324266, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324270, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324274, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324274, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324273, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324275, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324275, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324275, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324275, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324277, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324278, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324279, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324279, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324283, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324280, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324286, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324285, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324286, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324291, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324297, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324297, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324298, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324300, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324301, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324303, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324306, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324305, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324306, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324308, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324308, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324309, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324306, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324310, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324309, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324307, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324311, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324309, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324311, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324311, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324320, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324320, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324321, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324323, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324323, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324328, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324326, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324328, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324329, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324328, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324331, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324329, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324329, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324332, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324332, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324332, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324333, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324333, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324335, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324337, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324338, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324338, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324338, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324346, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324346, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324350, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324350, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324351, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324352, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324351, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324354, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324352, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324356, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324360, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324360, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324360, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324360, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324366, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324366, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324367, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324369, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324373, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324374, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881324381, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=32134259, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=201, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=True, env=False, epochs=14, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.005, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=1605, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=4, test_loader_workers=0, train_batch_size=16, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2503601338
:::MLLOG {"namespace": "", "time_ms": 1592881348420, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2503601338, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3748781206
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.005}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.005
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592881351965, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592881351965, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.005, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592881351965, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592881351965, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592881351965, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592881355251, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592881356324, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592881356324, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592881356576, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16384, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592881356577, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3948544, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592881356577, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 1605, 'decay_interval': 201, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1605
0: Scheduler decay interval: 201
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592881356577, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 201, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 1605, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881356578, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 998644676
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/241]	Time 0.390 (0.390)	Data 2.77e-01 (2.77e-01)	Tok/s 740 (740)	Loss/tok 10.5923 (10.5923)	LR 5.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/241]	Time 0.028 (0.090)	Data 6.44e-05 (2.53e-02)	Tok/s 38332 (27072)	Loss/tok 9.8909 (10.5460)	LR 5.610e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [0][20/241]	Time 0.022 (0.060)	Data 9.08e-05 (1.33e-02)	Tok/s 27248 (28959)	Loss/tok 9.1269 (10.0008)	LR 6.902e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: TRAIN [0][30/241]	Time 0.025 (0.050)	Data 6.79e-05 (9.01e-03)	Tok/s 41651 (29467)	Loss/tok 9.0979 (9.6985)	LR 8.491e-05
0: TRAIN [0][40/241]	Time 0.025 (0.044)	Data 6.32e-05 (6.83e-03)	Tok/s 25423 (28670)	Loss/tok 8.3088 (9.4781)	LR 1.069e-04
0: TRAIN [0][50/241]	Time 0.025 (0.040)	Data 6.32e-05 (5.50e-03)	Tok/s 25551 (28661)	Loss/tok 8.0569 (9.2594)	LR 1.346e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: TRAIN [0][60/241]	Time 0.025 (0.038)	Data 7.08e-05 (4.61e-03)	Tok/s 26745 (28745)	Loss/tok 8.4302 (9.0948)	LR 1.656e-04
0: TRAIN [0][70/241]	Time 0.025 (0.036)	Data 6.46e-05 (3.97e-03)	Tok/s 26026 (28921)	Loss/tok 7.5403 (8.9398)	LR 2.084e-04
0: TRAIN [0][80/241]	Time 0.025 (0.036)	Data 6.79e-05 (3.49e-03)	Tok/s 27300 (29569)	Loss/tok 7.9285 (8.8038)	LR 2.624e-04
0: TRAIN [0][90/241]	Time 0.025 (0.035)	Data 6.29e-05 (3.11e-03)	Tok/s 26284 (30183)	Loss/tok 7.5521 (8.6943)	LR 3.303e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 1.0
0: TRAIN [0][100/241]	Time 0.032 (0.034)	Data 5.84e-05 (2.81e-03)	Tok/s 43839 (30506)	Loss/tok 8.4783 (8.6113)	LR 4.064e-04
0: TRAIN [0][110/241]	Time 0.028 (0.033)	Data 6.56e-05 (2.56e-03)	Tok/s 37788 (31161)	Loss/tok 7.7021 (8.5339)	LR 5.116e-04
0: TRAIN [0][120/241]	Time 0.028 (0.033)	Data 6.68e-05 (2.36e-03)	Tok/s 36819 (31077)	Loss/tok 7.8816 (8.4708)	LR 6.441e-04
0: TRAIN [0][130/241]	Time 0.025 (0.033)	Data 6.70e-05 (2.18e-03)	Tok/s 24772 (31302)	Loss/tok 7.3670 (8.3981)	LR 8.109e-04
0: TRAIN [0][140/241]	Time 0.025 (0.032)	Data 6.65e-05 (2.03e-03)	Tok/s 28149 (31367)	Loss/tok 7.0119 (8.3331)	LR 1.021e-03
0: TRAIN [0][150/241]	Time 0.022 (0.032)	Data 6.27e-05 (1.90e-03)	Tok/s 14870 (31303)	Loss/tok 6.3802 (8.2679)	LR 1.285e-03
0: TRAIN [0][160/241]	Time 0.028 (0.031)	Data 6.29e-05 (1.79e-03)	Tok/s 36479 (31214)	Loss/tok 7.2451 (8.1997)	LR 1.618e-03
0: TRAIN [0][170/241]	Time 0.028 (0.031)	Data 6.39e-05 (1.69e-03)	Tok/s 38477 (31505)	Loss/tok 6.7642 (8.1073)	LR 2.037e-03
0: TRAIN [0][180/241]	Time 0.025 (0.031)	Data 6.29e-05 (1.60e-03)	Tok/s 27087 (31589)	Loss/tok 6.1780 (8.0316)	LR 2.564e-03
0: TRAIN [0][190/241]	Time 0.028 (0.031)	Data 6.58e-05 (1.52e-03)	Tok/s 38158 (31425)	Loss/tok 6.8568 (7.9650)	LR 3.228e-03
0: TRAIN [0][200/241]	Time 0.028 (0.031)	Data 6.77e-05 (1.45e-03)	Tok/s 37569 (31695)	Loss/tok 6.5562 (7.8761)	LR 4.064e-03
0: TRAIN [0][210/241]	Time 0.025 (0.030)	Data 6.53e-05 (1.38e-03)	Tok/s 27164 (31645)	Loss/tok 6.2666 (7.8006)	LR 5.000e-03
0: TRAIN [0][220/241]	Time 0.025 (0.030)	Data 6.87e-05 (1.32e-03)	Tok/s 24833 (31769)	Loss/tok 6.0891 (7.7181)	LR 5.000e-03
0: Upscaling, new scale: 2.0
0: TRAIN [0][230/241]	Time 0.025 (0.030)	Data 6.68e-05 (1.27e-03)	Tok/s 27472 (31967)	Loss/tok 5.4341 (7.6298)	LR 5.000e-03
0: TRAIN [0][240/241]	Time 0.032 (0.030)	Data 4.53e-05 (1.22e-03)	Tok/s 45127 (32082)	Loss/tok 5.8892 (7.5414)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881363862, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881363862, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.383 (0.383)	Decoder iters 149.0 (149.0)	Tok/s 2219 (2219)
0: Running moses detokenizer
0: BLEU(score=0.5618721802650287, counts=[14191, 1551, 254, 45], totals=[130594, 127591, 124588, 121591], precisions=[10.86650228953857, 1.2156029814015095, 0.20387196198670818, 0.0370093181238743], bp=1.0, sys_len=130594, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881364373, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.005600000000000001, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881364374, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 7.5425	Test BLEU: 0.56
0: Performance: Epoch: 0	Training: 32926438 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592881364374, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881364374, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881364374, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 3618663619
0: TRAIN [1][0/241]	Time 0.376 (0.376)	Data 2.87e-01 (2.87e-01)	Tok/s 1746 (1746)	Loss/tok 5.0877 (5.0877)	LR 5.000e-03
0: TRAIN [1][10/241]	Time 0.025 (0.057)	Data 6.99e-05 (2.62e-02)	Tok/s 27925 (25484)	Loss/tok 5.1399 (5.3221)	LR 5.000e-03
0: TRAIN [1][20/241]	Time 0.032 (0.043)	Data 7.08e-05 (1.37e-02)	Tok/s 46985 (28722)	Loss/tok 5.8611 (5.3926)	LR 5.000e-03
0: TRAIN [1][30/241]	Time 0.025 (0.038)	Data 7.01e-05 (9.33e-03)	Tok/s 25745 (30053)	Loss/tok 4.6983 (5.3417)	LR 5.000e-03
0: TRAIN [1][40/241]	Time 0.028 (0.035)	Data 6.84e-05 (7.07e-03)	Tok/s 38874 (29694)	Loss/tok 4.8829 (5.2030)	LR 5.000e-03
0: TRAIN [1][50/241]	Time 0.025 (0.033)	Data 6.87e-05 (5.70e-03)	Tok/s 27379 (30784)	Loss/tok 4.6227 (5.1533)	LR 5.000e-03
0: TRAIN [1][60/241]	Time 0.025 (0.032)	Data 6.94e-05 (4.78e-03)	Tok/s 26130 (30703)	Loss/tok 3.8235 (5.0569)	LR 5.000e-03
0: TRAIN [1][70/241]	Time 0.025 (0.031)	Data 7.49e-05 (4.12e-03)	Tok/s 25610 (30282)	Loss/tok 3.8099 (4.9849)	LR 5.000e-03
0: TRAIN [1][80/241]	Time 0.025 (0.030)	Data 7.18e-05 (3.62e-03)	Tok/s 26635 (30074)	Loss/tok 4.3119 (4.9097)	LR 5.000e-03
0: TRAIN [1][90/241]	Time 0.025 (0.030)	Data 6.79e-05 (3.23e-03)	Tok/s 26909 (30431)	Loss/tok 4.5703 (4.8311)	LR 5.000e-03
0: TRAIN [1][100/241]	Time 0.032 (0.030)	Data 6.79e-05 (2.92e-03)	Tok/s 47463 (31005)	Loss/tok 4.0466 (4.7736)	LR 5.000e-03
0: TRAIN [1][110/241]	Time 0.032 (0.030)	Data 6.72e-05 (2.66e-03)	Tok/s 47353 (31022)	Loss/tok 4.5095 (4.7285)	LR 5.000e-03
0: Upscaling, new scale: 4.0
0: TRAIN [1][120/241]	Time 0.025 (0.029)	Data 6.87e-05 (2.44e-03)	Tok/s 24675 (31201)	Loss/tok 3.9629 (4.6595)	LR 5.000e-03
0: TRAIN [1][130/241]	Time 0.032 (0.029)	Data 7.01e-05 (2.26e-03)	Tok/s 43821 (31571)	Loss/tok 5.1015 (4.6397)	LR 5.000e-03
0: TRAIN [1][140/241]	Time 0.025 (0.029)	Data 6.68e-05 (2.11e-03)	Tok/s 26501 (31990)	Loss/tok 3.6182 (4.6024)	LR 5.000e-03
0: TRAIN [1][150/241]	Time 0.029 (0.029)	Data 6.79e-05 (1.97e-03)	Tok/s 33867 (32144)	Loss/tok 4.1723 (4.5738)	LR 5.000e-03
0: TRAIN [1][160/241]	Time 0.026 (0.029)	Data 7.06e-05 (1.85e-03)	Tok/s 24342 (31944)	Loss/tok 3.3382 (4.5257)	LR 5.000e-03
0: TRAIN [1][170/241]	Time 0.028 (0.029)	Data 6.56e-05 (1.75e-03)	Tok/s 38695 (32190)	Loss/tok 4.0633 (4.4924)	LR 5.000e-03
0: TRAIN [1][180/241]	Time 0.033 (0.029)	Data 6.72e-05 (1.66e-03)	Tok/s 44854 (32439)	Loss/tok 4.0085 (4.4548)	LR 5.000e-03
0: TRAIN [1][190/241]	Time 0.025 (0.029)	Data 6.75e-05 (1.57e-03)	Tok/s 24332 (32086)	Loss/tok 3.8824 (4.4252)	LR 5.000e-03
0: TRAIN [1][200/241]	Time 0.025 (0.029)	Data 6.34e-05 (1.50e-03)	Tok/s 26029 (32301)	Loss/tok 3.4221 (4.3924)	LR 5.000e-03
0: TRAIN [1][210/241]	Time 0.032 (0.029)	Data 6.39e-05 (1.43e-03)	Tok/s 46497 (32515)	Loss/tok 3.9940 (4.3639)	LR 5.000e-03
0: TRAIN [1][220/241]	Time 0.028 (0.029)	Data 6.72e-05 (1.37e-03)	Tok/s 36934 (32588)	Loss/tok 4.3956 (4.3433)	LR 5.000e-03
0: TRAIN [1][230/241]	Time 0.025 (0.029)	Data 6.41e-05 (1.31e-03)	Tok/s 27488 (32543)	Loss/tok 3.2873 (4.3205)	LR 5.000e-03
0: TRAIN [1][240/241]	Time 0.025 (0.028)	Data 4.65e-05 (1.26e-03)	Tok/s 27749 (32381)	Loss/tok 3.1613 (4.3034)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881371276, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881371276, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.337 (0.337)	Decoder iters 149.0 (149.0)	Tok/s 2020 (2020)
0: Running moses detokenizer
0: BLEU(score=16.43215092147351, counts=[31599, 13401, 6779, 3579], totals=[65863, 62860, 59858, 56862], precisions=[47.97686106007926, 21.31880369074133, 11.325136155568178, 6.2941859238155535], bp=1.0, sys_len=65863, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881371780, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1643, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881371780, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 4.2678	Test BLEU: 16.43
0: Performance: Epoch: 1	Training: 33240628 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592881371780, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881371781, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881371781, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 4169673131
0: TRAIN [2][0/241]	Time 0.369 (0.369)	Data 2.83e-01 (2.83e-01)	Tok/s 1665 (1665)	Loss/tok 3.4952 (3.4952)	LR 5.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [2][10/241]	Time 0.028 (0.057)	Data 6.44e-05 (2.58e-02)	Tok/s 37308 (29263)	Loss/tok 3.8187 (3.6550)	LR 5.000e-03
0: TRAIN [2][20/241]	Time 0.028 (0.043)	Data 6.87e-05 (1.35e-02)	Tok/s 39590 (30996)	Loss/tok 3.3428 (3.6099)	LR 5.000e-03
0: TRAIN [2][30/241]	Time 0.028 (0.038)	Data 6.58e-05 (9.18e-03)	Tok/s 38066 (31227)	Loss/tok 3.5078 (3.6266)	LR 5.000e-03
0: TRAIN [2][40/241]	Time 0.025 (0.035)	Data 7.99e-05 (6.96e-03)	Tok/s 25269 (31891)	Loss/tok 2.9955 (3.5911)	LR 5.000e-03
0: TRAIN [2][50/241]	Time 0.028 (0.033)	Data 6.51e-05 (5.61e-03)	Tok/s 38690 (31912)	Loss/tok 3.5165 (3.5760)	LR 5.000e-03
0: TRAIN [2][60/241]	Time 0.028 (0.033)	Data 8.13e-05 (4.70e-03)	Tok/s 37298 (32876)	Loss/tok 3.8805 (3.5965)	LR 5.000e-03
0: TRAIN [2][70/241]	Time 0.028 (0.032)	Data 7.34e-05 (4.05e-03)	Tok/s 36762 (32810)	Loss/tok 3.8262 (3.6128)	LR 5.000e-03
0: TRAIN [2][80/241]	Time 0.028 (0.031)	Data 6.96e-05 (3.56e-03)	Tok/s 38028 (32910)	Loss/tok 3.2505 (3.6229)	LR 5.000e-03
0: TRAIN [2][90/241]	Time 0.025 (0.031)	Data 7.30e-05 (3.17e-03)	Tok/s 27300 (33114)	Loss/tok 3.5685 (3.6285)	LR 5.000e-03
0: TRAIN [2][100/241]	Time 0.025 (0.030)	Data 6.84e-05 (2.87e-03)	Tok/s 25955 (32535)	Loss/tok 3.2028 (3.6146)	LR 5.000e-03
0: TRAIN [2][110/241]	Time 0.025 (0.030)	Data 7.03e-05 (2.61e-03)	Tok/s 25687 (32825)	Loss/tok 3.3563 (3.6157)	LR 5.000e-03
0: TRAIN [2][120/241]	Time 0.025 (0.030)	Data 6.68e-05 (2.40e-03)	Tok/s 28394 (32816)	Loss/tok 3.7847 (3.6125)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [2][130/241]	Time 0.022 (0.030)	Data 6.89e-05 (2.23e-03)	Tok/s 15003 (32567)	Loss/tok 2.5641 (3.6070)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [2][140/241]	Time 0.025 (0.029)	Data 8.27e-05 (2.07e-03)	Tok/s 28946 (32502)	Loss/tok 3.6026 (3.6029)	LR 5.000e-03
0: TRAIN [2][150/241]	Time 0.028 (0.029)	Data 6.79e-05 (1.94e-03)	Tok/s 40548 (32666)	Loss/tok 3.1622 (3.5904)	LR 5.000e-03
0: TRAIN [2][160/241]	Time 0.028 (0.029)	Data 6.84e-05 (1.82e-03)	Tok/s 37298 (32717)	Loss/tok 3.2545 (3.5941)	LR 5.000e-03
0: TRAIN [2][170/241]	Time 0.025 (0.029)	Data 6.87e-05 (1.72e-03)	Tok/s 24918 (32420)	Loss/tok 3.1169 (3.5849)	LR 5.000e-03
0: TRAIN [2][180/241]	Time 0.025 (0.029)	Data 6.70e-05 (1.63e-03)	Tok/s 23820 (32269)	Loss/tok 3.2577 (3.5718)	LR 5.000e-03
0: TRAIN [2][190/241]	Time 0.032 (0.029)	Data 6.46e-05 (1.55e-03)	Tok/s 46313 (32655)	Loss/tok 3.6609 (3.5736)	LR 5.000e-03
0: TRAIN [2][200/241]	Time 0.025 (0.029)	Data 6.89e-05 (1.47e-03)	Tok/s 26478 (32576)	Loss/tok 3.2255 (3.5611)	LR 5.000e-03
0: TRAIN [2][210/241]	Time 0.028 (0.029)	Data 6.56e-05 (1.41e-03)	Tok/s 36425 (32717)	Loss/tok 3.6332 (3.5597)	LR 5.000e-03
0: TRAIN [2][220/241]	Time 0.032 (0.028)	Data 6.94e-05 (1.35e-03)	Tok/s 46650 (32505)	Loss/tok 4.2324 (3.5563)	LR 5.000e-03
0: TRAIN [2][230/241]	Time 0.032 (0.028)	Data 6.87e-05 (1.29e-03)	Tok/s 45081 (32774)	Loss/tok 3.6366 (3.5529)	LR 5.000e-03
0: TRAIN [2][240/241]	Time 0.025 (0.028)	Data 4.63e-05 (1.24e-03)	Tok/s 25536 (32551)	Loss/tok 3.1022 (3.5404)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881378659, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881378659, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.186 (0.186)	Decoder iters 77.0 (77.0)	Tok/s 3121 (3121)
0: Running moses detokenizer
0: BLEU(score=18.745614663394406, counts=[32183, 14697, 7820, 4294], totals=[58947, 55944, 52941, 49944], precisions=[54.596501942422854, 26.270913770913772, 14.771160348312272, 8.59762934486625], bp=0.9073844939948116, sys_len=58947, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881379092, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1875, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881379092, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.5767	Test BLEU: 18.75
0: Performance: Epoch: 2	Training: 33336554 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592881379092, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881379092, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881379093, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 1439412192
0: TRAIN [3][0/241]	Time 0.356 (0.356)	Data 3.12e-01 (3.12e-01)	Tok/s 4016 (4016)	Loss/tok 3.5974 (3.5974)	LR 5.000e-03
0: TRAIN [3][10/241]	Time 0.032 (0.058)	Data 6.63e-05 (2.84e-02)	Tok/s 45644 (33501)	Loss/tok 3.5703 (3.5040)	LR 5.000e-03
0: TRAIN [3][20/241]	Time 0.025 (0.043)	Data 6.75e-05 (1.49e-02)	Tok/s 24394 (31907)	Loss/tok 2.7322 (3.4552)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [3][30/241]	Time 0.025 (0.038)	Data 6.32e-05 (1.01e-02)	Tok/s 25938 (32443)	Loss/tok 3.1180 (3.4254)	LR 5.000e-03
0: TRAIN [3][40/241]	Time 0.032 (0.035)	Data 6.51e-05 (7.68e-03)	Tok/s 47004 (31582)	Loss/tok 3.5097 (3.4235)	LR 5.000e-03
0: TRAIN [3][50/241]	Time 0.025 (0.033)	Data 7.80e-05 (6.19e-03)	Tok/s 25205 (30793)	Loss/tok 3.2619 (3.4059)	LR 5.000e-03
0: TRAIN [3][60/241]	Time 0.022 (0.032)	Data 6.58e-05 (5.18e-03)	Tok/s 16545 (30479)	Loss/tok 2.8469 (3.4082)	LR 5.000e-03
0: TRAIN [3][70/241]	Time 0.025 (0.031)	Data 6.60e-05 (4.46e-03)	Tok/s 25888 (30762)	Loss/tok 3.0988 (3.4188)	LR 5.000e-03
0: TRAIN [3][80/241]	Time 0.025 (0.031)	Data 7.08e-05 (3.92e-03)	Tok/s 25094 (31000)	Loss/tok 2.7939 (3.3961)	LR 5.000e-03
0: TRAIN [3][90/241]	Time 0.025 (0.030)	Data 6.56e-05 (3.50e-03)	Tok/s 24969 (30992)	Loss/tok 3.1389 (3.4257)	LR 5.000e-03
0: TRAIN [3][100/241]	Time 0.025 (0.030)	Data 6.58e-05 (3.16e-03)	Tok/s 26598 (30904)	Loss/tok 3.1313 (3.4148)	LR 5.000e-03
0: TRAIN [3][110/241]	Time 0.028 (0.029)	Data 6.56e-05 (2.88e-03)	Tok/s 36021 (30751)	Loss/tok 3.1795 (3.4032)	LR 5.000e-03
0: TRAIN [3][120/241]	Time 0.028 (0.029)	Data 6.89e-05 (2.65e-03)	Tok/s 38094 (30902)	Loss/tok 3.8803 (3.4088)	LR 5.000e-03
0: TRAIN [3][130/241]	Time 0.022 (0.029)	Data 6.72e-05 (2.45e-03)	Tok/s 13604 (30869)	Loss/tok 2.5234 (3.4141)	LR 5.000e-03
0: TRAIN [3][140/241]	Time 0.025 (0.029)	Data 6.46e-05 (2.28e-03)	Tok/s 24810 (31107)	Loss/tok 3.0929 (3.4155)	LR 5.000e-03
0: TRAIN [3][150/241]	Time 0.025 (0.029)	Data 6.70e-05 (2.13e-03)	Tok/s 28596 (31061)	Loss/tok 2.7253 (3.3988)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [3][160/241]	Time 0.022 (0.029)	Data 6.87e-05 (2.01e-03)	Tok/s 15461 (31137)	Loss/tok 2.5173 (3.4009)	LR 5.000e-03
0: TRAIN [3][170/241]	Time 0.028 (0.028)	Data 6.58e-05 (1.89e-03)	Tok/s 37227 (31259)	Loss/tok 3.1755 (3.3940)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [3][180/241]	Time 0.032 (0.029)	Data 6.56e-05 (1.79e-03)	Tok/s 46270 (31795)	Loss/tok 3.4299 (3.4139)	LR 5.000e-03
0: TRAIN [3][190/241]	Time 0.032 (0.028)	Data 6.68e-05 (1.70e-03)	Tok/s 47328 (31762)	Loss/tok 3.2997 (3.4085)	LR 5.000e-03
0: TRAIN [3][200/241]	Time 0.032 (0.028)	Data 6.82e-05 (1.62e-03)	Tok/s 43201 (32078)	Loss/tok 4.0267 (3.4183)	LR 5.000e-03
0: TRAIN [3][210/241]	Time 0.025 (0.028)	Data 6.82e-05 (1.55e-03)	Tok/s 24421 (32096)	Loss/tok 3.1239 (3.4181)	LR 5.000e-03
0: TRAIN [3][220/241]	Time 0.022 (0.028)	Data 6.20e-05 (1.48e-03)	Tok/s 14294 (32196)	Loss/tok 2.3656 (3.4163)	LR 5.000e-03
0: TRAIN [3][230/241]	Time 0.025 (0.028)	Data 6.41e-05 (1.42e-03)	Tok/s 26731 (32162)	Loss/tok 3.3032 (3.4057)	LR 5.000e-03
0: TRAIN [3][240/241]	Time 0.036 (0.028)	Data 5.65e-05 (1.36e-03)	Tok/s 52961 (32478)	Loss/tok 3.7350 (3.4228)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881385954, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881385955, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.338 (0.338)	Decoder iters 149.0 (149.0)	Tok/s 2169 (2169)
0: Running moses detokenizer
0: BLEU(score=21.40315014345499, counts=[35101, 16685, 9103, 5143], totals=[64719, 61716, 58713, 55715], precisions=[54.23600488264652, 27.03512865383369, 15.504232452778771, 9.230907296060307], bp=1.0, sys_len=64719, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881386398, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.214, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881386398, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.3927	Test BLEU: 21.40
0: Performance: Epoch: 3	Training: 33249312 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592881386398, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881386398, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881386399, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 1227694472
0: TRAIN [4][0/241]	Time 0.359 (0.359)	Data 2.79e-01 (2.79e-01)	Tok/s 908 (908)	Loss/tok 2.5437 (2.5437)	LR 5.000e-03
0: TRAIN [4][10/241]	Time 0.032 (0.057)	Data 6.63e-05 (2.54e-02)	Tok/s 45818 (30176)	Loss/tok 3.5887 (3.2453)	LR 5.000e-03
0: TRAIN [4][20/241]	Time 0.032 (0.043)	Data 6.63e-05 (1.33e-02)	Tok/s 44953 (32040)	Loss/tok 3.7362 (3.2983)	LR 5.000e-03
0: TRAIN [4][30/241]	Time 0.025 (0.038)	Data 6.77e-05 (9.06e-03)	Tok/s 25368 (32064)	Loss/tok 2.9010 (3.2988)	LR 5.000e-03
0: TRAIN [4][40/241]	Time 0.028 (0.035)	Data 6.39e-05 (6.87e-03)	Tok/s 34717 (31367)	Loss/tok 3.5813 (3.2944)	LR 5.000e-03
0: TRAIN [4][50/241]	Time 0.030 (0.033)	Data 6.48e-05 (5.53e-03)	Tok/s 35285 (31737)	Loss/tok 3.5311 (3.3204)	LR 5.000e-03
0: TRAIN [4][60/241]	Time 0.028 (0.033)	Data 6.63e-05 (4.64e-03)	Tok/s 36948 (32746)	Loss/tok 3.2989 (3.3372)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [4][70/241]	Time 0.022 (0.032)	Data 6.72e-05 (3.99e-03)	Tok/s 16607 (32969)	Loss/tok 2.3486 (3.3378)	LR 5.000e-03
0: TRAIN [4][80/241]	Time 0.025 (0.031)	Data 6.39e-05 (3.51e-03)	Tok/s 27217 (32465)	Loss/tok 3.5492 (3.3301)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [4][90/241]	Time 0.032 (0.031)	Data 6.39e-05 (3.13e-03)	Tok/s 46679 (33361)	Loss/tok 3.5662 (3.3516)	LR 5.000e-03
0: TRAIN [4][100/241]	Time 0.028 (0.031)	Data 6.53e-05 (2.83e-03)	Tok/s 36628 (33409)	Loss/tok 3.2890 (3.3476)	LR 5.000e-03
0: TRAIN [4][110/241]	Time 0.028 (0.030)	Data 1.00e-04 (2.58e-03)	Tok/s 37100 (33194)	Loss/tok 3.6135 (3.3401)	LR 5.000e-03
0: TRAIN [4][120/241]	Time 0.025 (0.030)	Data 6.27e-05 (2.37e-03)	Tok/s 26804 (33108)	Loss/tok 2.8901 (3.3242)	LR 5.000e-03
0: TRAIN [4][130/241]	Time 0.028 (0.030)	Data 6.10e-05 (2.20e-03)	Tok/s 36453 (32711)	Loss/tok 3.1551 (3.3141)	LR 5.000e-03
0: TRAIN [4][140/241]	Time 0.028 (0.029)	Data 6.53e-05 (2.04e-03)	Tok/s 40658 (32827)	Loss/tok 3.1332 (3.3165)	LR 5.000e-03
0: TRAIN [4][150/241]	Time 0.028 (0.029)	Data 6.22e-05 (1.91e-03)	Tok/s 38664 (32888)	Loss/tok 2.9113 (3.3144)	LR 5.000e-03
0: TRAIN [4][160/241]	Time 0.025 (0.029)	Data 6.70e-05 (1.80e-03)	Tok/s 28017 (32797)	Loss/tok 2.7812 (3.3228)	LR 5.000e-03
0: TRAIN [4][170/241]	Time 0.025 (0.029)	Data 6.46e-05 (1.70e-03)	Tok/s 25943 (32812)	Loss/tok 3.3006 (3.3248)	LR 5.000e-03
0: TRAIN [4][180/241]	Time 0.028 (0.029)	Data 6.53e-05 (1.61e-03)	Tok/s 38645 (32724)	Loss/tok 3.1700 (3.3200)	LR 5.000e-03
0: TRAIN [4][190/241]	Time 0.025 (0.029)	Data 6.77e-05 (1.53e-03)	Tok/s 26653 (32644)	Loss/tok 2.6618 (3.3330)	LR 5.000e-03
0: TRAIN [4][200/241]	Time 0.028 (0.029)	Data 6.75e-05 (1.45e-03)	Tok/s 38190 (32858)	Loss/tok 2.9978 (3.3329)	LR 5.000e-03
0: TRAIN [4][210/241]	Time 0.025 (0.029)	Data 6.87e-05 (1.39e-03)	Tok/s 25284 (32795)	Loss/tok 2.9368 (3.3292)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [4][220/241]	Time 0.032 (0.029)	Data 6.48e-05 (1.33e-03)	Tok/s 44401 (32781)	Loss/tok 3.5259 (3.3263)	LR 5.000e-03
0: TRAIN [4][230/241]	Time 0.025 (0.028)	Data 6.51e-05 (1.27e-03)	Tok/s 25175 (32642)	Loss/tok 2.8666 (3.3224)	LR 5.000e-03
0: TRAIN [4][240/241]	Time 0.025 (0.028)	Data 4.27e-05 (1.22e-03)	Tok/s 25788 (32568)	Loss/tok 3.0994 (3.3137)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881393276, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881393276, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.335 (0.335)	Decoder iters 149.0 (149.0)	Tok/s 2148 (2148)
0: Running moses detokenizer
0: BLEU(score=22.48342383611885, counts=[35687, 17314, 9620, 5573], totals=[63528, 60525, 57522, 54525], precisions=[56.175229819921924, 28.606361007847998, 16.72403602100066, 10.220999541494727], bp=0.9820915253732243, sys_len=63528, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881393720, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2248, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881393720, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.3002	Test BLEU: 22.48
0: Performance: Epoch: 4	Training: 33347946 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1592881393720, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881393721, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881393721, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 3735970034
0: TRAIN [5][0/241]	Time 0.356 (0.356)	Data 2.62e-01 (2.62e-01)	Tok/s 1822 (1822)	Loss/tok 2.6990 (2.6990)	LR 5.000e-03
0: TRAIN [5][10/241]	Time 0.028 (0.057)	Data 7.01e-05 (2.39e-02)	Tok/s 36748 (30437)	Loss/tok 3.1254 (3.2733)	LR 5.000e-03
0: TRAIN [5][20/241]	Time 0.025 (0.043)	Data 6.65e-05 (1.25e-02)	Tok/s 26080 (32626)	Loss/tok 2.8281 (3.2295)	LR 5.000e-03
0: TRAIN [5][30/241]	Time 0.025 (0.039)	Data 6.99e-05 (8.52e-03)	Tok/s 27887 (34235)	Loss/tok 2.9397 (3.3087)	LR 5.000e-03
0: TRAIN [5][40/241]	Time 0.025 (0.036)	Data 6.96e-05 (6.46e-03)	Tok/s 25723 (33443)	Loss/tok 3.3792 (3.3171)	LR 5.000e-03
0: TRAIN [5][50/241]	Time 0.028 (0.034)	Data 6.94e-05 (5.21e-03)	Tok/s 37236 (32922)	Loss/tok 3.4359 (3.3039)	LR 5.000e-03
0: TRAIN [5][60/241]	Time 0.028 (0.033)	Data 9.44e-05 (4.36e-03)	Tok/s 38728 (33124)	Loss/tok 3.4457 (3.3035)	LR 5.000e-03
0: TRAIN [5][70/241]	Time 0.036 (0.032)	Data 6.41e-05 (3.76e-03)	Tok/s 55148 (33549)	Loss/tok 3.1792 (3.2823)	LR 5.000e-03
0: TRAIN [5][80/241]	Time 0.032 (0.032)	Data 6.70e-05 (3.30e-03)	Tok/s 47258 (33888)	Loss/tok 3.2339 (3.2940)	LR 5.000e-03
0: TRAIN [5][90/241]	Time 0.025 (0.031)	Data 6.72e-05 (2.95e-03)	Tok/s 26120 (34072)	Loss/tok 3.1406 (3.3068)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][100/241]	Time 0.025 (0.031)	Data 6.96e-05 (2.66e-03)	Tok/s 24612 (34231)	Loss/tok 3.0319 (3.2976)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [5][110/241]	Time 0.022 (0.030)	Data 6.72e-05 (2.43e-03)	Tok/s 15244 (33412)	Loss/tok 2.4673 (3.2899)	LR 5.000e-03
0: TRAIN [5][120/241]	Time 0.025 (0.030)	Data 6.41e-05 (2.23e-03)	Tok/s 23887 (33159)	Loss/tok 2.9852 (3.2806)	LR 5.000e-03
0: TRAIN [5][130/241]	Time 0.025 (0.030)	Data 6.63e-05 (2.07e-03)	Tok/s 25031 (32897)	Loss/tok 2.7481 (3.2662)	LR 5.000e-03
0: TRAIN [5][140/241]	Time 0.028 (0.029)	Data 6.51e-05 (1.93e-03)	Tok/s 39661 (32900)	Loss/tok 3.6931 (3.2592)	LR 5.000e-03
0: TRAIN [5][150/241]	Time 0.025 (0.029)	Data 8.99e-05 (1.80e-03)	Tok/s 25862 (33003)	Loss/tok 3.1175 (3.2578)	LR 5.000e-03
0: TRAIN [5][160/241]	Time 0.022 (0.029)	Data 6.68e-05 (1.70e-03)	Tok/s 14192 (32936)	Loss/tok 2.4087 (3.2482)	LR 5.000e-03
0: TRAIN [5][170/241]	Time 0.028 (0.029)	Data 6.32e-05 (1.60e-03)	Tok/s 38746 (32893)	Loss/tok 2.9909 (3.2422)	LR 5.000e-03
0: TRAIN [5][180/241]	Time 0.028 (0.029)	Data 6.63e-05 (1.52e-03)	Tok/s 36504 (32815)	Loss/tok 3.1988 (3.2369)	LR 5.000e-03
0: TRAIN [5][190/241]	Time 0.025 (0.029)	Data 6.65e-05 (1.44e-03)	Tok/s 25852 (32920)	Loss/tok 3.1695 (3.2532)	LR 5.000e-03
0: TRAIN [5][200/241]	Time 0.022 (0.029)	Data 6.58e-05 (1.37e-03)	Tok/s 15847 (32727)	Loss/tok 2.8248 (3.2545)	LR 5.000e-03
0: TRAIN [5][210/241]	Time 0.032 (0.029)	Data 6.68e-05 (1.31e-03)	Tok/s 46134 (32726)	Loss/tok 3.4871 (3.2549)	LR 5.000e-03
0: TRAIN [5][220/241]	Time 0.025 (0.028)	Data 6.79e-05 (1.25e-03)	Tok/s 27710 (32612)	Loss/tok 2.9586 (3.2586)	LR 5.000e-03
0: TRAIN [5][230/241]	Time 0.022 (0.028)	Data 6.79e-05 (1.20e-03)	Tok/s 14960 (32613)	Loss/tok 2.7429 (3.2540)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [5][240/241]	Time 0.022 (0.028)	Data 4.27e-05 (1.16e-03)	Tok/s 14959 (32544)	Loss/tok 2.7903 (3.2470)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881400591, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881400591, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.306 (0.306)	Decoder iters 135.0 (135.0)	Tok/s 2529 (2529)
0: Running moses detokenizer
0: BLEU(score=21.752197067320694, counts=[36071, 17397, 9646, 5570], totals=[66889, 63886, 60883, 57885], precisions=[53.92665460688603, 27.231318285696396, 15.843503112527307, 9.622527425066943], bp=1.0, sys_len=66889, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881401039, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2175, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881401039, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.2314	Test BLEU: 21.75
0: Performance: Epoch: 5	Training: 33306298 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1592881401039, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881401039, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 7, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881401039, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 7}}
0: Starting epoch 6
0: Sampler for epoch 6 uses seed 2356553846
0: TRAIN [6][0/241]	Time 0.363 (0.363)	Data 2.67e-01 (2.67e-01)	Tok/s 2906 (2906)	Loss/tok 3.1091 (3.1091)	LR 5.000e-03
0: TRAIN [6][10/241]	Time 0.032 (0.057)	Data 6.56e-05 (2.44e-02)	Tok/s 43911 (26876)	Loss/tok 3.4111 (3.0223)	LR 5.000e-03
0: TRAIN [6][20/241]	Time 0.036 (0.042)	Data 7.18e-05 (1.28e-02)	Tok/s 51929 (28586)	Loss/tok 3.9042 (3.1968)	LR 5.000e-03
0: TRAIN [6][30/241]	Time 0.025 (0.037)	Data 6.84e-05 (8.69e-03)	Tok/s 26259 (29514)	Loss/tok 3.4121 (3.2034)	LR 5.000e-03
0: TRAIN [6][40/241]	Time 0.032 (0.035)	Data 6.37e-05 (6.58e-03)	Tok/s 46380 (30437)	Loss/tok 3.5831 (3.2708)	LR 5.000e-03
0: TRAIN [6][50/241]	Time 0.032 (0.034)	Data 6.68e-05 (5.31e-03)	Tok/s 44782 (31486)	Loss/tok 3.8884 (3.2723)	LR 5.000e-03
0: TRAIN [6][60/241]	Time 0.028 (0.033)	Data 6.44e-05 (4.45e-03)	Tok/s 37054 (32223)	Loss/tok 3.2559 (3.2960)	LR 5.000e-03
0: TRAIN [6][70/241]	Time 0.022 (0.032)	Data 6.39e-05 (3.83e-03)	Tok/s 15869 (32465)	Loss/tok 2.6514 (3.2798)	LR 5.000e-03
0: TRAIN [6][80/241]	Time 0.032 (0.031)	Data 6.51e-05 (3.37e-03)	Tok/s 45887 (31935)	Loss/tok 3.7410 (3.2652)	LR 5.000e-03
0: TRAIN [6][90/241]	Time 0.028 (0.031)	Data 6.87e-05 (3.00e-03)	Tok/s 37498 (31827)	Loss/tok 2.6780 (3.2611)	LR 5.000e-03
0: TRAIN [6][100/241]	Time 0.028 (0.030)	Data 6.87e-05 (2.71e-03)	Tok/s 37653 (31964)	Loss/tok 3.1747 (3.2666)	LR 5.000e-03
0: TRAIN [6][110/241]	Time 0.028 (0.030)	Data 6.63e-05 (2.47e-03)	Tok/s 37878 (32562)	Loss/tok 2.9576 (3.2793)	LR 5.000e-03
0: TRAIN [6][120/241]	Time 0.028 (0.030)	Data 6.65e-05 (2.28e-03)	Tok/s 34831 (32224)	Loss/tok 3.4119 (3.2784)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [6][130/241]	Time 0.025 (0.030)	Data 6.75e-05 (2.11e-03)	Tok/s 25946 (32427)	Loss/tok 2.9216 (3.2683)	LR 5.000e-03
0: TRAIN [6][140/241]	Time 0.036 (0.029)	Data 6.72e-05 (1.96e-03)	Tok/s 50789 (32446)	Loss/tok 3.8457 (3.2680)	LR 5.000e-03
0: TRAIN [6][150/241]	Time 0.028 (0.029)	Data 6.70e-05 (1.84e-03)	Tok/s 37760 (32527)	Loss/tok 3.4380 (3.2604)	LR 5.000e-03
0: TRAIN [6][160/241]	Time 0.028 (0.029)	Data 6.89e-05 (1.73e-03)	Tok/s 38480 (32754)	Loss/tok 3.1068 (3.2500)	LR 5.000e-03
0: TRAIN [6][170/241]	Time 0.025 (0.029)	Data 6.63e-05 (1.63e-03)	Tok/s 25614 (32307)	Loss/tok 2.9429 (3.2382)	LR 5.000e-03
0: TRAIN [6][180/241]	Time 0.022 (0.029)	Data 6.53e-05 (1.54e-03)	Tok/s 14569 (32337)	Loss/tok 2.4165 (3.2327)	LR 2.500e-03
0: TRAIN [6][190/241]	Time 0.025 (0.029)	Data 6.84e-05 (1.47e-03)	Tok/s 27988 (32398)	Loss/tok 2.9145 (3.2258)	LR 2.500e-03
0: TRAIN [6][200/241]	Time 0.022 (0.029)	Data 6.37e-05 (1.40e-03)	Tok/s 15760 (32269)	Loss/tok 2.7473 (3.2154)	LR 2.500e-03
0: TRAIN [6][210/241]	Time 0.025 (0.029)	Data 6.41e-05 (1.33e-03)	Tok/s 25184 (32282)	Loss/tok 3.2058 (3.2184)	LR 2.500e-03
0: TRAIN [6][220/241]	Time 0.032 (0.028)	Data 6.44e-05 (1.28e-03)	Tok/s 45950 (32384)	Loss/tok 3.4154 (3.2129)	LR 2.500e-03
0: TRAIN [6][230/241]	Time 0.028 (0.028)	Data 8.46e-05 (1.22e-03)	Tok/s 38324 (32468)	Loss/tok 3.3070 (3.2095)	LR 2.500e-03
0: TRAIN [6][240/241]	Time 0.025 (0.028)	Data 4.17e-05 (1.18e-03)	Tok/s 27337 (32390)	Loss/tok 3.5086 (3.2095)	LR 2.500e-03
:::MLLOG {"namespace": "", "time_ms": 1592881407916, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881407916, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 7}}
0: Running evaluation on test set
0: TEST [6][0/1]	Time 0.236 (0.236)	Decoder iters 100.0 (100.0)	Tok/s 3095 (3095)
0: Running moses detokenizer
0: BLEU(score=23.830923835808814, counts=[36782, 18278, 10327, 6093], totals=[64777, 61774, 58771, 55772], precisions=[56.78249996140605, 29.58850001618804, 17.571591431148015, 10.92483683568816], bp=1.0, sys_len=64777, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881408289, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23829999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881408289, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 7}}
0: Summary: Epoch: 6	Training Loss: 3.1792	Test BLEU: 23.83
0: Performance: Epoch: 6	Training: 33250022 Tok/s
0: Finished epoch 6
:::MLLOG {"namespace": "", "time_ms": 1592881408289, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881408289, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 8, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881408289, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 8}}
0: Starting epoch 7
0: Sampler for epoch 7 uses seed 1720992587
0: TRAIN [7][0/241]	Time 0.368 (0.368)	Data 2.59e-01 (2.59e-01)	Tok/s 5105 (5105)	Loss/tok 3.1789 (3.1789)	LR 2.500e-03
0: Upscaling, new scale: 64.0
0: TRAIN [7][10/241]	Time 0.025 (0.058)	Data 6.63e-05 (2.36e-02)	Tok/s 26125 (31708)	Loss/tok 2.9830 (3.0328)	LR 2.500e-03
0: TRAIN [7][20/241]	Time 0.028 (0.043)	Data 6.79e-05 (1.24e-02)	Tok/s 36370 (32926)	Loss/tok 3.2805 (3.0638)	LR 2.500e-03
0: TRAIN [7][30/241]	Time 0.022 (0.037)	Data 6.79e-05 (8.41e-03)	Tok/s 16564 (30969)	Loss/tok 2.5871 (3.0028)	LR 2.500e-03
0: TRAIN [7][40/241]	Time 0.036 (0.035)	Data 6.65e-05 (6.38e-03)	Tok/s 52453 (30365)	Loss/tok 3.3433 (3.0116)	LR 2.500e-03
0: TRAIN [7][50/241]	Time 0.025 (0.033)	Data 6.53e-05 (5.14e-03)	Tok/s 28689 (30453)	Loss/tok 3.0595 (3.0265)	LR 2.500e-03
0: TRAIN [7][60/241]	Time 0.028 (0.032)	Data 6.51e-05 (4.31e-03)	Tok/s 39695 (30930)	Loss/tok 2.6605 (3.0032)	LR 2.500e-03
0: TRAIN [7][70/241]	Time 0.028 (0.031)	Data 6.39e-05 (3.71e-03)	Tok/s 36485 (30998)	Loss/tok 2.9023 (3.0058)	LR 2.500e-03
0: TRAIN [7][80/241]	Time 0.025 (0.031)	Data 6.32e-05 (3.26e-03)	Tok/s 26382 (31303)	Loss/tok 2.9818 (3.0205)	LR 2.500e-03
0: TRAIN [7][90/241]	Time 0.028 (0.030)	Data 6.60e-05 (2.91e-03)	Tok/s 38892 (31128)	Loss/tok 2.9621 (3.0119)	LR 2.500e-03
0: TRAIN [7][100/241]	Time 0.025 (0.030)	Data 6.46e-05 (2.63e-03)	Tok/s 24012 (31638)	Loss/tok 3.0996 (3.0326)	LR 2.500e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [7][110/241]	Time 0.028 (0.030)	Data 7.75e-05 (2.40e-03)	Tok/s 37020 (31868)	Loss/tok 3.1318 (3.0342)	LR 2.500e-03
0: TRAIN [7][120/241]	Time 0.028 (0.030)	Data 6.46e-05 (2.20e-03)	Tok/s 34969 (32359)	Loss/tok 2.9500 (3.0352)	LR 2.500e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [7][130/241]	Time 0.028 (0.029)	Data 6.60e-05 (2.04e-03)	Tok/s 36820 (32486)	Loss/tok 3.1036 (3.0475)	LR 2.500e-03
0: TRAIN [7][140/241]	Time 0.022 (0.029)	Data 6.65e-05 (1.90e-03)	Tok/s 15343 (32636)	Loss/tok 2.8584 (3.0568)	LR 1.250e-03
0: TRAIN [7][150/241]	Time 0.025 (0.029)	Data 9.25e-05 (1.78e-03)	Tok/s 26534 (32614)	Loss/tok 3.0808 (3.0654)	LR 1.250e-03
0: TRAIN [7][160/241]	Time 0.025 (0.029)	Data 6.44e-05 (1.67e-03)	Tok/s 27115 (32408)	Loss/tok 3.0123 (3.0598)	LR 1.250e-03
0: TRAIN [7][170/241]	Time 0.032 (0.029)	Data 6.37e-05 (1.58e-03)	Tok/s 47225 (32416)	Loss/tok 2.9377 (3.0591)	LR 1.250e-03
0: TRAIN [7][180/241]	Time 0.032 (0.029)	Data 6.65e-05 (1.50e-03)	Tok/s 45195 (32735)	Loss/tok 3.2992 (3.0692)	LR 1.250e-03
0: TRAIN [7][190/241]	Time 0.022 (0.029)	Data 6.51e-05 (1.42e-03)	Tok/s 14675 (32710)	Loss/tok 2.3698 (3.0622)	LR 1.250e-03
0: TRAIN [7][200/241]	Time 0.025 (0.029)	Data 6.51e-05 (1.35e-03)	Tok/s 26730 (32696)	Loss/tok 2.5934 (3.0609)	LR 1.250e-03
0: TRAIN [7][210/241]	Time 0.022 (0.029)	Data 6.58e-05 (1.29e-03)	Tok/s 14597 (32871)	Loss/tok 2.3029 (3.0680)	LR 1.250e-03
0: TRAIN [7][220/241]	Time 0.028 (0.028)	Data 6.51e-05 (1.24e-03)	Tok/s 38665 (32589)	Loss/tok 3.1734 (3.0608)	LR 1.250e-03
0: TRAIN [7][230/241]	Time 0.025 (0.028)	Data 6.70e-05 (1.19e-03)	Tok/s 26711 (32571)	Loss/tok 2.9611 (3.0688)	LR 1.250e-03
0: TRAIN [7][240/241]	Time 0.025 (0.028)	Data 4.72e-05 (1.14e-03)	Tok/s 26274 (32572)	Loss/tok 2.9758 (3.0715)	LR 1.250e-03
:::MLLOG {"namespace": "", "time_ms": 1592881415159, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881415159, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 8}}
0: Running evaluation on test set
0: TEST [7][0/1]	Time 0.255 (0.255)	Decoder iters 111.0 (111.0)	Tok/s 2804 (2804)
0: Running moses detokenizer
0: BLEU(score=23.849101565462405, counts=[37046, 18473, 10480, 6168], totals=[65406, 62403, 59400, 56401], precisions=[56.64006360272758, 29.60274345784658, 17.643097643097644, 10.935976312476729], bp=1.0, sys_len=65406, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881415521, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23850000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881415521, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 8}}
0: Summary: Epoch: 7	Training Loss: 3.0820	Test BLEU: 23.85
0: Performance: Epoch: 7	Training: 33226486 Tok/s
0: Finished epoch 7
:::MLLOG {"namespace": "", "time_ms": 1592881415521, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881415522, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 9, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881415522, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 9}}
0: Starting epoch 8
0: Sampler for epoch 8 uses seed 1786145463
0: TRAIN [8][0/241]	Time 0.360 (0.360)	Data 2.84e-01 (2.84e-01)	Tok/s 4342 (4342)	Loss/tok 2.7795 (2.7795)	LR 1.250e-03
0: Upscaling, new scale: 32.0
0: TRAIN [8][10/241]	Time 0.032 (0.058)	Data 6.48e-05 (2.59e-02)	Tok/s 46100 (31248)	Loss/tok 3.2046 (3.0102)	LR 1.250e-03
0: TRAIN [8][20/241]	Time 0.025 (0.044)	Data 6.48e-05 (1.36e-02)	Tok/s 27756 (33562)	Loss/tok 2.4493 (2.9767)	LR 1.250e-03
0: TRAIN [8][30/241]	Time 0.032 (0.038)	Data 7.13e-05 (9.24e-03)	Tok/s 45480 (31796)	Loss/tok 2.8570 (2.9385)	LR 1.250e-03
0: TRAIN [8][40/241]	Time 0.028 (0.036)	Data 8.46e-05 (7.00e-03)	Tok/s 38510 (33598)	Loss/tok 3.0070 (2.9686)	LR 1.250e-03
0: TRAIN [8][50/241]	Time 0.028 (0.034)	Data 7.08e-05 (5.64e-03)	Tok/s 35094 (34155)	Loss/tok 3.2698 (2.9986)	LR 1.250e-03
0: TRAIN [8][60/241]	Time 0.025 (0.033)	Data 9.20e-05 (4.73e-03)	Tok/s 26037 (33931)	Loss/tok 2.9485 (2.9871)	LR 1.250e-03
0: TRAIN [8][70/241]	Time 0.025 (0.032)	Data 6.84e-05 (4.07e-03)	Tok/s 25036 (33364)	Loss/tok 2.8226 (2.9812)	LR 1.250e-03
0: TRAIN [8][80/241]	Time 0.032 (0.032)	Data 7.27e-05 (3.58e-03)	Tok/s 45692 (33721)	Loss/tok 2.9114 (3.0108)	LR 1.250e-03
0: TRAIN [8][90/241]	Time 0.028 (0.031)	Data 6.94e-05 (3.19e-03)	Tok/s 36900 (33757)	Loss/tok 3.0041 (3.0209)	LR 1.250e-03
0: TRAIN [8][100/241]	Time 0.028 (0.031)	Data 9.23e-05 (2.89e-03)	Tok/s 37171 (33292)	Loss/tok 3.3030 (3.0150)	LR 6.250e-04
0: TRAIN [8][110/241]	Time 0.025 (0.030)	Data 7.13e-05 (2.63e-03)	Tok/s 24550 (33452)	Loss/tok 2.4101 (3.0207)	LR 6.250e-04
0: TRAIN [8][120/241]	Time 0.025 (0.030)	Data 7.01e-05 (2.42e-03)	Tok/s 29057 (33146)	Loss/tok 2.7716 (3.0224)	LR 6.250e-04
0: TRAIN [8][130/241]	Time 0.032 (0.030)	Data 6.58e-05 (2.24e-03)	Tok/s 45094 (33049)	Loss/tok 3.0806 (3.0173)	LR 6.250e-04
0: Upscaling, new scale: 64.0
0: TRAIN [8][140/241]	Time 0.025 (0.030)	Data 6.56e-05 (2.09e-03)	Tok/s 26907 (33097)	Loss/tok 2.7251 (3.0269)	LR 6.250e-04
0: TRAIN [8][150/241]	Time 0.028 (0.029)	Data 6.68e-05 (1.95e-03)	Tok/s 36903 (32994)	Loss/tok 3.0262 (3.0281)	LR 6.250e-04
0: TRAIN [8][160/241]	Time 0.022 (0.029)	Data 7.77e-05 (1.84e-03)	Tok/s 16928 (32729)	Loss/tok 2.7130 (3.0274)	LR 6.250e-04
0: TRAIN [8][170/241]	Time 0.032 (0.029)	Data 6.63e-05 (1.73e-03)	Tok/s 44105 (32904)	Loss/tok 3.2686 (3.0267)	LR 6.250e-04
0: TRAIN [8][180/241]	Time 0.028 (0.029)	Data 6.56e-05 (1.64e-03)	Tok/s 36341 (32926)	Loss/tok 3.0539 (3.0325)	LR 6.250e-04
0: TRAIN [8][190/241]	Time 0.025 (0.029)	Data 6.41e-05 (1.56e-03)	Tok/s 28535 (32506)	Loss/tok 2.6154 (3.0290)	LR 6.250e-04
0: TRAIN [8][200/241]	Time 0.024 (0.029)	Data 9.18e-05 (1.48e-03)	Tok/s 28489 (32562)	Loss/tok 3.0261 (3.0283)	LR 6.250e-04
0: TRAIN [8][210/241]	Time 0.028 (0.029)	Data 6.65e-05 (1.42e-03)	Tok/s 37490 (32371)	Loss/tok 3.2386 (3.0238)	LR 6.250e-04
0: TRAIN [8][220/241]	Time 0.036 (0.028)	Data 6.56e-05 (1.36e-03)	Tok/s 48906 (32328)	Loss/tok 3.2898 (3.0294)	LR 6.250e-04
0: TRAIN [8][230/241]	Time 0.025 (0.028)	Data 6.53e-05 (1.30e-03)	Tok/s 26314 (32338)	Loss/tok 2.7837 (3.0371)	LR 6.250e-04
0: TRAIN [8][240/241]	Time 0.036 (0.028)	Data 4.43e-05 (1.25e-03)	Tok/s 53143 (32354)	Loss/tok 3.1862 (3.0426)	LR 6.250e-04
:::MLLOG {"namespace": "", "time_ms": 1592881422396, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 9}}
:::MLLOG {"namespace": "", "time_ms": 1592881422396, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 9}}
0: Running evaluation on test set
0: TEST [8][0/1]	Time 0.269 (0.269)	Decoder iters 115.0 (115.0)	Tok/s 2690 (2690)
0: Running moses detokenizer
0: BLEU(score=24.23388439248848, counts=[37102, 18605, 10657, 6323], totals=[65196, 62193, 59190, 56193], precisions=[56.9083992882999, 29.91494219606708, 18.004730528805542, 11.252291210649012], bp=1.0, sys_len=65196, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881422816, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24230000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 9}}
:::MLLOG {"namespace": "", "time_ms": 1592881422816, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 9}}
0: Summary: Epoch: 8	Training Loss: 3.0322	Test BLEU: 24.23
0: Performance: Epoch: 8	Training: 33200706 Tok/s
0: Finished epoch 8
:::MLLOG {"namespace": "", "time_ms": 1592881422816, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 9}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592881422816, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:49 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 08:02:01 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:03:50 PM
RESULT,RNN_TRANSLATOR,,109,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/21791/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/89587/status failed: No such process
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/94063/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/94072/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/18781/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
slurmstepd: error: _is_a_lwp: open() /proc/17289/status failed: No such file or directory
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/32422/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/89756/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/8296/status failed: No such file or directory
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/27418/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/16039/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/15296/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
slurmstepd: error: _is_a_lwp: open() /proc/90635/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:52 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
ENDING TIMING RUN AT 2020-06-22 08:03:53 PM
RESULT,RNN_TRANSLATOR,,112,nvidia,2020-06-22 08:02:01 PM
