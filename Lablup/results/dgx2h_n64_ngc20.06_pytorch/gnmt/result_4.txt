+ echo 'Beginning trial 5 of 5'
Beginning trial 5 of 5
+ srun --ntasks=64 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592880280989, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592880281021, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592880281021, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592880281021, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592880281021, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "64xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=64 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n030
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n006
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n096
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n098
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n044
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n010
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n026
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n012
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n034
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n013
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n007
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n025
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n018
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n015
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n039
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n009
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n045
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n033
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n027
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n035
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n005
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n095
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n011
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n055
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n042
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n024
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n028
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n032
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n052
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n060
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n016
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n056
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n008
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n020
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n022
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n029
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n003
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n049
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n017
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n019
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n021
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
vm.drop_caches = 3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n004
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n036
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n048
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n002
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n040
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n014
Clearing cache on circe-n058
Clearing cache on circe-n054
Clearing cache on circe-n031
Clearing cache on circe-n001
Clearing cache on circe-n057
Clearing cache on circe-n023
Clearing cache on circe-n047
Clearing cache on circe-n051
Clearing cache on circe-n053
Clearing cache on circe-n097
Clearing cache on circe-n043
Clearing cache on circe-n059
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=64 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592880287932, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880287936, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880287949, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880287975, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880287988, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880287989, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288000, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288007, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288012, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288020, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288022, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288028, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288029, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288032, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288033, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288033, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288032, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288035, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288037, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288040, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288040, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288043, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288048, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288064, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288064, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288071, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288071, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288072, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288072, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288072, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288074, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288076, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288080, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288080, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288080, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288080, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288082, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288083, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288084, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288083, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288086, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288090, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288099, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288100, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288105, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288104, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288108, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288118, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288127, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288127, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288129, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288132, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288137, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288138, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288138, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288139, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288144, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288143, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288153, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288158, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288165, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288168, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288191, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880288212, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=1024 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/14043860/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 15 ']'
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
running benchmark
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' -n 9 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 7 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 1 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ '[' -n 3 ']'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 5 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 13 ']'
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ echo 'running benchmark'
+ TARGET=24.0
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 10 ']'
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 15 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ '[' -n 6 ']'
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ '[' -n 1 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 11 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 14 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ '[' -n 0 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ declare -a CMD
+ '[' -n 14 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 6 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
running benchmark
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 10 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
running benchmark
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 5 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 1 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 15 ']'
running benchmark
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 9 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 4 ']'
running benchmark
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 12 ']'
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 12 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' -n 15 ']'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 7 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 13 ']'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 3 ']'
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 8 ']'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 2 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 3 ']'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 12 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' -n 4 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 2 ']'
+ MATH=fp16
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 8 ']'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 11 ']'
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
running benchmark
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 5 ']'
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 1 ']'
running benchmark
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 10 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 3 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 13 ']'
+ TARGET=24.0
running benchmark
+ '[' -n 10 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
running benchmark
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ echo 'running benchmark'
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 7 ']'
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 8 ']'
+ '[' -n 15 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
running benchmark
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 9 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 10 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
running benchmark
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 3 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 0 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 11 ']'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 2 ']'
running benchmark
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 14 ']'
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
running benchmark
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 15 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 8 ']'
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 6 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ declare -a CMD
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 8 ']'
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ '[' -n 0 ']'
+ '[' -n 8 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ '[' -n 12 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
running benchmark
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ declare -a CMD
+ '[' -n 15 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ '[' -n 11 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' -n 11 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 0 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
running benchmark
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
running benchmark
+ DIST_OPTS=
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 6 ']'
+ TARGET=24.0
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' -n 3 ']'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DIST_OPTS=
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ declare -a CMD
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
running benchmark
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 15 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ declare -a CMD
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ DIST_OPTS=
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ MATH=fp16
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ WARMUP_STEPS=200
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ '[' -n 4 ']'
+ LR=5.0e-3
+ TARGET=24.0
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ '[' -n 4 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ TARGET=24.0
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 4 ']'
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 2 ']'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
+ '[' -n 9 ']'
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ '[' -n 11 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 8 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 12 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=14
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ DIST_OPTS=
+ TARGET=24.0
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ NUMEPOCHS=14
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 7 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ '[' -n 15 ']'
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' -n 8 ']'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 13 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' -n 14 ']'
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ '[' -n 13 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ LR=5.0e-3
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
running benchmark
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 7 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ LR=5.0e-3
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ '[' -n 1 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
running benchmark
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 1 ']'
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 5 ']'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ '[' -n 13 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 5 ']'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 1 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ '[' -n 4 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 4 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ declare -a CMD
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 13 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 11 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ DATASET_DIR=/data
+ '[' -n 15 ']'
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 11 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
running benchmark
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 8 ']'
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ '[' -n 14 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 9 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 15 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 7 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 2 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ TARGET=24.0
+ NUMEPOCHS=14
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 6 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ NUMEPOCHS=14
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 7 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 4 ']'
running benchmark
+ echo 'running benchmark'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
running benchmark
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 13 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 15 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 6 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ LR=5.0e-3
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 2 ']'
running benchmark
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 3 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
running benchmark
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ '[' -n 8 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ declare -a CMD
+ TARGET=24.0
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 2 ']'
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' -n 6 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 13 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 14 ']'
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 0 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ DIST_OPTS=
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 10 ']'
+ LR=5.0e-3
+ REMAIN_STEPS=1605
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ '[' -n 12 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
running benchmark
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
running benchmark
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 10 ']'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 14 ']'
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 13 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
running benchmark
+ '[' -n 10 ']'
+ LR=5.0e-3
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 9 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ '[' -n 4 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 0 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 7 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
running benchmark
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 3 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ '[' -n 11 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ LR=5.0e-3
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
running benchmark
+ NUMEPOCHS=14
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ TARGET=24.0
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ MATH=fp16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ declare -a CMD
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 3 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 14 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ '[' -n 11 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 3 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
running benchmark
+ NUMEPOCHS=14
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 7 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 12 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 10 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 15 ']'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 0 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ '[' -n 1 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 2 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 10 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 1 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ declare -a CMD
+ '[' -n 6 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DIST_OPTS=
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 1 ']'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ '[' -n 12 ']'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ '[' -n 15 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
running benchmark
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 15 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ '[' -n 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ LR=5.0e-3
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:44:51 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292902, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292937, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880292941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292953, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292954, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292963, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292971, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292971, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292976, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292977, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292987, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292987, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880292996, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293016, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293040, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293044, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293052, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293071, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293100, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293117, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293117, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293126, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293129, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293144, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293144, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293164, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293165, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293198, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293220, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293240, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293241, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293241, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293246, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293250, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293261, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293261, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293271, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293292, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293298, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293303, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293311, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293317, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293318, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293330, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293341, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293344, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293382, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293385, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293396, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293419, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293422, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293434, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293468, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293480, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293481, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293487, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293499, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293506, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293508, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293516, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293516, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293516, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293517, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293519, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293520, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293521, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293525, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293526, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293532, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293534, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293536, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293540, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293547, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293545, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293548, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293550, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293550, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293553, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293555, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293553, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293557, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293557, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293559, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293558, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293559, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293561, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293563, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293567, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293573, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293575, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293575, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293576, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293579, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293582, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293585, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293585, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293583, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293573, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293586, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293586, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293587, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293589, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293588, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293590, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293592, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293592, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293592, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293591, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293594, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293599, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293599, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293598, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293604, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293604, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293604, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293604, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293614, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293615, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293618, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293619, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293618, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293622, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293622, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293623, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293624, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293626, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293625, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293628, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293626, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293629, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293634, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293634, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293634, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293638, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293642, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293643, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293640, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293640, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293643, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293642, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293642, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293642, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293646, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293646, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293647, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293647, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293650, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293647, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293650, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293651, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293650, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293651, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293651, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293654, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293654, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293660, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293660, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293660, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293661, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293661, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293660, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293663, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293675, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293675, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293681, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293682, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293686, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293695, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293695, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293700, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293700, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293700, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293700, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293716, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293716, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293727, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293735, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293735, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293738, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293739, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293738, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293746, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293757, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293757, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293773, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293773, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293791, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293799, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293823, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293823, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293823, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293838, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293838, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293853, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293877, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293878, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293883, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293884, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293889, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293902, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293921, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293925, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293930, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293934, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293934, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293934, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293938, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293939, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293940, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293939, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293944, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293944, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293950, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293951, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293955, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293955, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293956, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293959, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293965, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880293978, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=32134259, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=201, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=True, env=False, epochs=14, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.005, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=1605, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=4, test_loader_workers=0, train_batch_size=16, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 17036472
:::MLLOG {"namespace": "", "time_ms": 1592880317254, "event_type": "POINT_IN_TIME", "key": "seed", "value": 17036472, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1212527942
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.005}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.005
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592880320716, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592880320717, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.005, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592880320717, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592880320717, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592880320717, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592880323929, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592880325140, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592880325140, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592880325376, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16384, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592880325377, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3948544, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592880325377, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 1605, 'decay_interval': 201, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1605
0: Scheduler decay interval: 201
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592880325378, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592880325378, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592880325378, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 201, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592880325378, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592880325378, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592880325379, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 1605, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592880325379, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592880325379, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880325379, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 2738469894
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/241]	Time 0.381 (0.381)	Data 2.66e-01 (2.66e-01)	Tok/s 2616 (2616)	Loss/tok 10.6897 (10.6897)	LR 5.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/241]	Time 0.030 (0.098)	Data 8.03e-05 (2.43e-02)	Tok/s 22163 (26856)	Loss/tok 9.8417 (10.5266)	LR 5.610e-05
0: TRAIN [0][20/241]	Time 0.025 (0.064)	Data 6.79e-05 (1.27e-02)	Tok/s 26490 (28886)	Loss/tok 8.9914 (10.0535)	LR 7.063e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [0][30/241]	Time 0.025 (0.052)	Data 6.99e-05 (8.66e-03)	Tok/s 25117 (29224)	Loss/tok 8.5922 (9.7310)	LR 8.689e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: TRAIN [0][40/241]	Time 0.022 (0.046)	Data 6.87e-05 (6.56e-03)	Tok/s 13046 (29339)	Loss/tok 7.9541 (9.4888)	LR 1.069e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1.0
0: TRAIN [0][50/241]	Time 0.025 (0.042)	Data 8.92e-05 (5.29e-03)	Tok/s 26961 (30372)	Loss/tok 8.3714 (9.3288)	LR 1.285e-04
0: TRAIN [0][60/241]	Time 0.028 (0.039)	Data 6.63e-05 (4.44e-03)	Tok/s 37054 (30084)	Loss/tok 7.9963 (9.1645)	LR 1.618e-04
0: TRAIN [0][70/241]	Time 0.028 (0.037)	Data 7.68e-05 (3.82e-03)	Tok/s 38640 (29725)	Loss/tok 7.9494 (9.0254)	LR 2.037e-04
0: TRAIN [0][80/241]	Time 0.028 (0.036)	Data 6.41e-05 (3.36e-03)	Tok/s 40914 (30255)	Loss/tok 8.0932 (8.8824)	LR 2.564e-04
0: TRAIN [0][90/241]	Time 0.022 (0.035)	Data 6.48e-05 (3.00e-03)	Tok/s 13656 (30317)	Loss/tok 7.1159 (8.7941)	LR 3.228e-04
0: TRAIN [0][100/241]	Time 0.022 (0.034)	Data 6.46e-05 (2.71e-03)	Tok/s 14089 (30353)	Loss/tok 7.0451 (8.7097)	LR 4.064e-04
0: TRAIN [0][110/241]	Time 0.028 (0.034)	Data 6.96e-05 (2.47e-03)	Tok/s 36856 (30712)	Loss/tok 7.8490 (8.6301)	LR 5.116e-04
0: TRAIN [0][120/241]	Time 0.028 (0.033)	Data 6.79e-05 (2.27e-03)	Tok/s 37288 (31139)	Loss/tok 7.8471 (8.5549)	LR 6.441e-04
0: TRAIN [0][130/241]	Time 0.025 (0.033)	Data 6.63e-05 (2.10e-03)	Tok/s 24193 (30991)	Loss/tok 7.2966 (8.4942)	LR 8.109e-04
0: TRAIN [0][140/241]	Time 0.028 (0.032)	Data 6.84e-05 (1.96e-03)	Tok/s 38044 (31263)	Loss/tok 7.7831 (8.4342)	LR 1.021e-03
0: TRAIN [0][150/241]	Time 0.028 (0.032)	Data 7.32e-05 (1.83e-03)	Tok/s 37375 (31622)	Loss/tok 7.4760 (8.3596)	LR 1.285e-03
0: TRAIN [0][160/241]	Time 0.025 (0.032)	Data 6.91e-05 (1.73e-03)	Tok/s 27808 (31612)	Loss/tok 6.9828 (8.2847)	LR 1.618e-03
0: TRAIN [0][170/241]	Time 0.036 (0.032)	Data 6.60e-05 (1.63e-03)	Tok/s 52397 (32101)	Loss/tok 7.1455 (8.1857)	LR 2.037e-03
0: Upscaling, new scale: 2.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1.0
0: TRAIN [0][180/241]	Time 0.025 (0.031)	Data 6.46e-05 (1.54e-03)	Tok/s 23331 (31834)	Loss/tok 6.5237 (8.1290)	LR 2.506e-03
0: TRAIN [0][190/241]	Time 0.025 (0.031)	Data 6.94e-05 (1.47e-03)	Tok/s 25080 (31843)	Loss/tok 6.1665 (8.0529)	LR 3.155e-03
0: TRAIN [0][200/241]	Time 0.028 (0.031)	Data 6.94e-05 (1.40e-03)	Tok/s 38802 (31925)	Loss/tok 6.3682 (7.9727)	LR 3.972e-03
0: TRAIN [0][210/241]	Time 0.032 (0.031)	Data 6.44e-05 (1.33e-03)	Tok/s 48412 (32006)	Loss/tok 6.3282 (7.8949)	LR 5.000e-03
0: TRAIN [0][220/241]	Time 0.025 (0.031)	Data 6.58e-05 (1.28e-03)	Tok/s 25648 (32003)	Loss/tok 5.8298 (7.8161)	LR 5.000e-03
0: TRAIN [0][230/241]	Time 0.025 (0.030)	Data 6.75e-05 (1.22e-03)	Tok/s 26342 (31952)	Loss/tok 5.8766 (7.7441)	LR 5.000e-03
0: TRAIN [0][240/241]	Time 0.025 (0.030)	Data 4.63e-05 (1.18e-03)	Tok/s 26302 (32099)	Loss/tok 5.3071 (7.6586)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880332715, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880332716, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.341 (0.341)	Decoder iters 149.0 (149.0)	Tok/s 1334 (1334)
0: Running moses detokenizer
0: BLEU(score=0.7913822700357593, counts=[10707, 1008, 148, 35], totals=[47798, 44795, 41793, 38793], precisions=[22.400518850161095, 2.250251144100904, 0.35412628909147464, 0.0902224628154564], bp=0.7024992054346034, sys_len=47798, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880333210, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.0079, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880333211, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 7.6455	Test BLEU: 0.79
0: Performance: Epoch: 0	Training: 32916650 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592880333211, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880333211, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880333211, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 3496712112
0: TRAIN [1][0/241]	Time 0.363 (0.363)	Data 2.70e-01 (2.70e-01)	Tok/s 1811 (1811)	Loss/tok 5.2844 (5.2844)	LR 5.000e-03
0: TRAIN [1][10/241]	Time 0.021 (0.056)	Data 6.91e-05 (2.46e-02)	Tok/s 15074 (24877)	Loss/tok 4.4728 (5.4007)	LR 5.000e-03
0: TRAIN [1][20/241]	Time 0.025 (0.042)	Data 6.34e-05 (1.29e-02)	Tok/s 23913 (29039)	Loss/tok 4.8920 (5.4370)	LR 5.000e-03
0: TRAIN [1][30/241]	Time 0.032 (0.038)	Data 6.68e-05 (8.76e-03)	Tok/s 45955 (31393)	Loss/tok 5.6517 (5.4142)	LR 5.000e-03
0: TRAIN [1][40/241]	Time 0.032 (0.035)	Data 6.79e-05 (6.64e-03)	Tok/s 44653 (31685)	Loss/tok 5.3336 (5.3737)	LR 5.000e-03
0: TRAIN [1][50/241]	Time 0.036 (0.034)	Data 6.84e-05 (5.35e-03)	Tok/s 52235 (32697)	Loss/tok 5.1782 (5.3176)	LR 5.000e-03
0: TRAIN [1][60/241]	Time 0.025 (0.033)	Data 6.70e-05 (4.49e-03)	Tok/s 25023 (32914)	Loss/tok 4.4839 (5.2664)	LR 5.000e-03
0: Upscaling, new scale: 2.0
0: TRAIN [1][70/241]	Time 0.022 (0.032)	Data 6.70e-05 (3.86e-03)	Tok/s 14675 (33024)	Loss/tok 3.2332 (5.1855)	LR 5.000e-03
0: TRAIN [1][80/241]	Time 0.025 (0.031)	Data 6.75e-05 (3.40e-03)	Tok/s 26993 (32521)	Loss/tok 4.0327 (5.1099)	LR 5.000e-03
0: TRAIN [1][90/241]	Time 0.025 (0.031)	Data 7.41e-05 (3.03e-03)	Tok/s 25022 (32047)	Loss/tok 3.5963 (5.0342)	LR 5.000e-03
0: TRAIN [1][100/241]	Time 0.032 (0.030)	Data 7.03e-05 (2.74e-03)	Tok/s 47915 (32040)	Loss/tok 4.7235 (4.9709)	LR 5.000e-03
0: TRAIN [1][110/241]	Time 0.028 (0.030)	Data 6.68e-05 (2.50e-03)	Tok/s 38458 (32112)	Loss/tok 3.8937 (4.9199)	LR 5.000e-03
0: TRAIN [1][120/241]	Time 0.025 (0.030)	Data 6.77e-05 (2.30e-03)	Tok/s 25737 (32071)	Loss/tok 3.8693 (4.8642)	LR 5.000e-03
0: TRAIN [1][130/241]	Time 0.028 (0.030)	Data 6.29e-05 (2.13e-03)	Tok/s 36474 (32376)	Loss/tok 4.5796 (4.8112)	LR 5.000e-03
0: TRAIN [1][140/241]	Time 0.032 (0.030)	Data 6.94e-05 (1.98e-03)	Tok/s 43678 (32598)	Loss/tok 4.3091 (4.7524)	LR 5.000e-03
0: TRAIN [1][150/241]	Time 0.022 (0.029)	Data 6.32e-05 (1.85e-03)	Tok/s 15303 (32157)	Loss/tok 3.6522 (4.7101)	LR 5.000e-03
0: TRAIN [1][160/241]	Time 0.037 (0.029)	Data 8.87e-05 (1.74e-03)	Tok/s 51464 (32371)	Loss/tok 5.0791 (4.6723)	LR 5.000e-03
0: TRAIN [1][170/241]	Time 0.032 (0.029)	Data 6.58e-05 (1.64e-03)	Tok/s 47623 (32302)	Loss/tok 4.2681 (4.6369)	LR 5.000e-03
0: TRAIN [1][180/241]	Time 0.026 (0.029)	Data 6.46e-05 (1.56e-03)	Tok/s 22027 (32516)	Loss/tok 4.1379 (4.6021)	LR 5.000e-03
0: Upscaling, new scale: 4.0
0: TRAIN [1][190/241]	Time 0.025 (0.029)	Data 6.46e-05 (1.48e-03)	Tok/s 25292 (32276)	Loss/tok 3.6603 (4.5714)	LR 5.000e-03
0: TRAIN [1][200/241]	Time 0.022 (0.029)	Data 6.60e-05 (1.41e-03)	Tok/s 14527 (32350)	Loss/tok 3.2525 (4.5366)	LR 5.000e-03
0: TRAIN [1][210/241]	Time 0.025 (0.029)	Data 6.39e-05 (1.35e-03)	Tok/s 26200 (32340)	Loss/tok 3.5480 (4.5133)	LR 5.000e-03
0: TRAIN [1][220/241]	Time 0.028 (0.029)	Data 8.61e-05 (1.29e-03)	Tok/s 38227 (32409)	Loss/tok 4.1877 (4.4870)	LR 5.000e-03
0: TRAIN [1][230/241]	Time 0.025 (0.029)	Data 6.34e-05 (1.23e-03)	Tok/s 25972 (32322)	Loss/tok 3.3893 (4.4525)	LR 5.000e-03
0: TRAIN [1][240/241]	Time 0.022 (0.029)	Data 4.43e-05 (1.19e-03)	Tok/s 15521 (32323)	Loss/tok 2.8108 (4.4211)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880340142, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592880340142, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.231 (0.231)	Decoder iters 97.0 (97.0)	Tok/s 2691 (2691)
0: Running moses detokenizer
0: BLEU(score=14.427496401283019, counts=[28169, 11718, 5793, 2938], totals=[55874, 52871, 49868, 46871], precisions=[50.4152199591939, 22.163378789884813, 11.616668003529318, 6.268268225555247], bp=0.854248605234303, sys_len=55874, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880340637, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.14429999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592880340637, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 4.4036	Test BLEU: 14.43
0: Performance: Epoch: 1	Training: 33037266 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592880340637, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592880340638, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880340638, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3719676563
0: TRAIN [2][0/241]	Time 0.364 (0.364)	Data 2.64e-01 (2.64e-01)	Tok/s 2865 (2865)	Loss/tok 3.6944 (3.6944)	LR 5.000e-03
0: TRAIN [2][10/241]	Time 0.025 (0.056)	Data 6.91e-05 (2.41e-02)	Tok/s 27106 (23851)	Loss/tok 3.6862 (3.3903)	LR 5.000e-03
0: TRAIN [2][20/241]	Time 0.028 (0.042)	Data 6.70e-05 (1.27e-02)	Tok/s 34484 (28067)	Loss/tok 3.8597 (3.5887)	LR 5.000e-03
0: TRAIN [2][30/241]	Time 0.022 (0.037)	Data 6.53e-05 (8.59e-03)	Tok/s 15405 (27652)	Loss/tok 3.0536 (3.5767)	LR 5.000e-03
0: TRAIN [2][40/241]	Time 0.028 (0.034)	Data 6.87e-05 (6.51e-03)	Tok/s 36074 (29243)	Loss/tok 3.9400 (3.6392)	LR 5.000e-03
0: TRAIN [2][50/241]	Time 0.036 (0.033)	Data 7.53e-05 (5.25e-03)	Tok/s 49938 (28950)	Loss/tok 4.8628 (3.6903)	LR 5.000e-03
0: TRAIN [2][60/241]	Time 0.032 (0.032)	Data 7.37e-05 (4.40e-03)	Tok/s 46388 (29250)	Loss/tok 3.5058 (3.6902)	LR 5.000e-03
0: TRAIN [2][70/241]	Time 0.025 (0.031)	Data 6.63e-05 (3.79e-03)	Tok/s 25181 (29562)	Loss/tok 3.6900 (3.6792)	LR 5.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [2][80/241]	Time 0.022 (0.030)	Data 6.82e-05 (3.33e-03)	Tok/s 15852 (29426)	Loss/tok 2.5596 (3.6467)	LR 5.000e-03
0: TRAIN [2][90/241]	Time 0.025 (0.030)	Data 7.46e-05 (2.97e-03)	Tok/s 27832 (30211)	Loss/tok 3.5996 (3.6633)	LR 5.000e-03
0: TRAIN [2][100/241]	Time 0.025 (0.030)	Data 6.48e-05 (2.69e-03)	Tok/s 27149 (30580)	Loss/tok 3.0291 (3.6825)	LR 5.000e-03
0: TRAIN [2][110/241]	Time 0.036 (0.030)	Data 6.56e-05 (2.45e-03)	Tok/s 49320 (30867)	Loss/tok 4.2268 (3.6926)	LR 5.000e-03
0: TRAIN [2][120/241]	Time 0.025 (0.030)	Data 6.56e-05 (2.25e-03)	Tok/s 25243 (31146)	Loss/tok 3.1756 (3.6830)	LR 5.000e-03
0: TRAIN [2][130/241]	Time 0.022 (0.029)	Data 6.58e-05 (2.09e-03)	Tok/s 15310 (31339)	Loss/tok 2.7653 (3.6882)	LR 5.000e-03
0: TRAIN [2][140/241]	Time 0.036 (0.029)	Data 6.72e-05 (1.94e-03)	Tok/s 51591 (31687)	Loss/tok 3.7550 (3.6785)	LR 5.000e-03
0: TRAIN [2][150/241]	Time 0.032 (0.029)	Data 7.82e-05 (1.82e-03)	Tok/s 42858 (31896)	Loss/tok 4.0478 (3.6891)	LR 5.000e-03
0: TRAIN [2][160/241]	Time 0.028 (0.029)	Data 7.13e-05 (1.71e-03)	Tok/s 39055 (32166)	Loss/tok 3.6818 (3.6903)	LR 5.000e-03
0: TRAIN [2][170/241]	Time 0.025 (0.029)	Data 6.48e-05 (1.61e-03)	Tok/s 27726 (32135)	Loss/tok 3.5875 (3.6837)	LR 5.000e-03
0: TRAIN [2][180/241]	Time 0.025 (0.029)	Data 6.44e-05 (1.53e-03)	Tok/s 23711 (31935)	Loss/tok 3.1557 (3.6735)	LR 5.000e-03
0: TRAIN [2][190/241]	Time 0.025 (0.029)	Data 6.48e-05 (1.45e-03)	Tok/s 24679 (32011)	Loss/tok 3.4329 (3.6762)	LR 5.000e-03
0: TRAIN [2][200/241]	Time 0.028 (0.029)	Data 6.65e-05 (1.38e-03)	Tok/s 36806 (32080)	Loss/tok 3.5247 (3.6695)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [2][210/241]	Time 0.025 (0.029)	Data 6.39e-05 (1.32e-03)	Tok/s 26509 (32198)	Loss/tok 3.5120 (3.6555)	LR 5.000e-03
0: TRAIN [2][220/241]	Time 0.025 (0.028)	Data 6.48e-05 (1.26e-03)	Tok/s 25957 (32000)	Loss/tok 2.9753 (3.6446)	LR 5.000e-03
0: TRAIN [2][230/241]	Time 0.025 (0.028)	Data 6.51e-05 (1.21e-03)	Tok/s 26142 (32132)	Loss/tok 3.5344 (3.6429)	LR 5.000e-03
0: TRAIN [2][240/241]	Time 0.025 (0.028)	Data 4.27e-05 (1.17e-03)	Tok/s 24619 (32329)	Loss/tok 3.3601 (3.6396)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880347537, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592880347537, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.204 (0.204)	Decoder iters 86.0 (86.0)	Tok/s 3181 (3181)
0: Running moses detokenizer
0: BLEU(score=19.955227081407998, counts=[33903, 15587, 8371, 4650], totals=[63679, 60676, 57674, 54678], precisions=[53.240471741076334, 25.688905003625816, 14.514339216978188, 8.504334467244595], bp=0.9844652753316142, sys_len=63679, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880347970, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1996, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592880347970, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.6054	Test BLEU: 19.96
0: Performance: Epoch: 2	Training: 33138846 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592880347970, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592880347971, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880347971, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 603794055
0: TRAIN [3][0/241]	Time 0.366 (0.366)	Data 2.73e-01 (2.73e-01)	Tok/s 1653 (1653)	Loss/tok 3.2105 (3.2105)	LR 5.000e-03
0: TRAIN [3][10/241]	Time 0.028 (0.059)	Data 7.06e-05 (2.49e-02)	Tok/s 36787 (33068)	Loss/tok 3.3289 (3.4434)	LR 5.000e-03
0: TRAIN [3][20/241]	Time 0.024 (0.043)	Data 6.89e-05 (1.31e-02)	Tok/s 25871 (30844)	Loss/tok 3.4008 (3.3808)	LR 5.000e-03
0: TRAIN [3][30/241]	Time 0.028 (0.038)	Data 6.63e-05 (8.89e-03)	Tok/s 36485 (31965)	Loss/tok 3.1449 (3.4203)	LR 5.000e-03
0: TRAIN [3][40/241]	Time 0.022 (0.035)	Data 6.87e-05 (6.73e-03)	Tok/s 14375 (31669)	Loss/tok 2.6117 (3.4148)	LR 5.000e-03
0: TRAIN [3][50/241]	Time 0.025 (0.033)	Data 6.41e-05 (5.43e-03)	Tok/s 27665 (30534)	Loss/tok 3.4515 (3.3891)	LR 5.000e-03
0: TRAIN [3][60/241]	Time 0.028 (0.032)	Data 6.79e-05 (4.55e-03)	Tok/s 37271 (30618)	Loss/tok 3.2482 (3.3793)	LR 5.000e-03
0: TRAIN [3][70/241]	Time 0.028 (0.032)	Data 7.01e-05 (3.92e-03)	Tok/s 34151 (31126)	Loss/tok 3.2832 (3.3950)	LR 5.000e-03
0: TRAIN [3][80/241]	Time 0.022 (0.031)	Data 6.75e-05 (3.44e-03)	Tok/s 15343 (31106)	Loss/tok 2.7694 (3.4208)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [3][90/241]	Time 0.028 (0.031)	Data 6.63e-05 (3.07e-03)	Tok/s 37697 (31543)	Loss/tok 3.4304 (3.4364)	LR 5.000e-03
0: TRAIN [3][100/241]	Time 0.025 (0.030)	Data 6.72e-05 (2.78e-03)	Tok/s 26012 (32096)	Loss/tok 2.9385 (3.4057)	LR 5.000e-03
0: TRAIN [3][110/241]	Time 0.028 (0.030)	Data 8.23e-05 (2.53e-03)	Tok/s 38344 (32643)	Loss/tok 3.2422 (3.4112)	LR 5.000e-03
0: TRAIN [3][120/241]	Time 0.028 (0.030)	Data 6.72e-05 (2.33e-03)	Tok/s 37773 (32649)	Loss/tok 3.1133 (3.4104)	LR 5.000e-03
0: TRAIN [3][130/241]	Time 0.028 (0.030)	Data 6.70e-05 (2.16e-03)	Tok/s 35694 (32486)	Loss/tok 3.4944 (3.3988)	LR 5.000e-03
0: TRAIN [3][140/241]	Time 0.028 (0.030)	Data 6.84e-05 (2.01e-03)	Tok/s 38805 (32327)	Loss/tok 3.4490 (3.3944)	LR 5.000e-03
0: TRAIN [3][150/241]	Time 0.025 (0.029)	Data 6.70e-05 (1.88e-03)	Tok/s 28151 (32379)	Loss/tok 3.2223 (3.3935)	LR 5.000e-03
0: TRAIN [3][160/241]	Time 0.036 (0.029)	Data 6.72e-05 (1.77e-03)	Tok/s 53479 (32499)	Loss/tok 3.4660 (3.3917)	LR 5.000e-03
0: TRAIN [3][170/241]	Time 0.032 (0.029)	Data 6.58e-05 (1.67e-03)	Tok/s 43597 (32619)	Loss/tok 4.0402 (3.4024)	LR 5.000e-03
0: TRAIN [3][180/241]	Time 0.028 (0.029)	Data 6.77e-05 (1.58e-03)	Tok/s 39230 (32736)	Loss/tok 3.3774 (3.4006)	LR 5.000e-03
0: TRAIN [3][190/241]	Time 0.025 (0.029)	Data 6.79e-05 (1.50e-03)	Tok/s 26943 (32731)	Loss/tok 3.1848 (3.3983)	LR 5.000e-03
0: TRAIN [3][200/241]	Time 0.025 (0.029)	Data 6.65e-05 (1.43e-03)	Tok/s 25204 (32734)	Loss/tok 3.1273 (3.4058)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [3][210/241]	Time 0.025 (0.029)	Data 7.01e-05 (1.37e-03)	Tok/s 28406 (32637)	Loss/tok 3.0006 (3.3966)	LR 5.000e-03
0: TRAIN [3][220/241]	Time 0.025 (0.029)	Data 7.03e-05 (1.31e-03)	Tok/s 28500 (32489)	Loss/tok 3.1530 (3.3934)	LR 5.000e-03
0: TRAIN [3][230/241]	Time 0.028 (0.029)	Data 6.65e-05 (1.25e-03)	Tok/s 36423 (32574)	Loss/tok 3.4643 (3.3902)	LR 5.000e-03
0: TRAIN [3][240/241]	Time 0.022 (0.028)	Data 4.51e-05 (1.20e-03)	Tok/s 13960 (32444)	Loss/tok 2.7594 (3.3850)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880354875, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880354875, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.319 (0.319)	Decoder iters 141.0 (141.0)	Tok/s 2333 (2333)
0: Running moses detokenizer
0: BLEU(score=21.42786886074075, counts=[34877, 16590, 9094, 5185], totals=[63055, 60052, 57050, 54052], precisions=[55.31202918087384, 27.626057416905347, 15.94040315512708, 9.592614519351736], bp=0.9746199142893149, sys_len=63055, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880355310, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2143, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880355310, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.4068	Test BLEU: 21.43
0: Performance: Epoch: 3	Training: 33180652 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592880355310, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880355310, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880355310, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 3282536542
0: TRAIN [4][0/241]	Time 0.368 (0.368)	Data 2.64e-01 (2.64e-01)	Tok/s 1736 (1736)	Loss/tok 2.7346 (2.7346)	LR 5.000e-03
0: TRAIN [4][10/241]	Time 0.025 (0.057)	Data 7.37e-05 (2.41e-02)	Tok/s 26440 (28942)	Loss/tok 3.2939 (3.2118)	LR 5.000e-03
0: TRAIN [4][20/241]	Time 0.032 (0.044)	Data 6.79e-05 (1.26e-02)	Tok/s 44998 (33445)	Loss/tok 3.6628 (3.3524)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [4][30/241]	Time 0.028 (0.038)	Data 6.32e-05 (8.59e-03)	Tok/s 35897 (32721)	Loss/tok 3.6501 (3.3219)	LR 5.000e-03
0: TRAIN [4][40/241]	Time 0.032 (0.035)	Data 6.84e-05 (6.51e-03)	Tok/s 43662 (32365)	Loss/tok 4.0223 (3.3784)	LR 5.000e-03
0: TRAIN [4][50/241]	Time 0.028 (0.034)	Data 6.58e-05 (5.25e-03)	Tok/s 35141 (32911)	Loss/tok 3.2699 (3.3846)	LR 5.000e-03
0: TRAIN [4][60/241]	Time 0.025 (0.033)	Data 6.65e-05 (4.40e-03)	Tok/s 26898 (33460)	Loss/tok 3.0283 (3.3723)	LR 5.000e-03
0: TRAIN [4][70/241]	Time 0.025 (0.032)	Data 6.99e-05 (3.79e-03)	Tok/s 25254 (33348)	Loss/tok 3.2239 (3.3456)	LR 5.000e-03
0: TRAIN [4][80/241]	Time 0.036 (0.032)	Data 6.87e-05 (3.33e-03)	Tok/s 52572 (33981)	Loss/tok 3.5398 (3.3569)	LR 5.000e-03
0: TRAIN [4][90/241]	Time 0.028 (0.031)	Data 6.65e-05 (2.97e-03)	Tok/s 37886 (34261)	Loss/tok 3.8843 (3.3883)	LR 5.000e-03
0: TRAIN [4][100/241]	Time 0.028 (0.031)	Data 6.94e-05 (2.69e-03)	Tok/s 36911 (34276)	Loss/tok 3.9642 (3.3846)	LR 5.000e-03
0: TRAIN [4][110/241]	Time 0.025 (0.031)	Data 6.48e-05 (2.45e-03)	Tok/s 26199 (34002)	Loss/tok 3.1589 (3.3796)	LR 5.000e-03
0: TRAIN [4][120/241]	Time 0.025 (0.030)	Data 7.03e-05 (2.25e-03)	Tok/s 25073 (33729)	Loss/tok 2.5020 (3.3806)	LR 5.000e-03
0: TRAIN [4][130/241]	Time 0.025 (0.030)	Data 6.89e-05 (2.09e-03)	Tok/s 26998 (33547)	Loss/tok 2.5422 (3.3602)	LR 5.000e-03
0: TRAIN [4][140/241]	Time 0.025 (0.030)	Data 8.54e-05 (1.94e-03)	Tok/s 28561 (33458)	Loss/tok 2.7688 (3.3515)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [4][150/241]	Time 0.028 (0.030)	Data 6.79e-05 (1.82e-03)	Tok/s 36495 (33684)	Loss/tok 3.5442 (3.3545)	LR 5.000e-03
0: TRAIN [4][160/241]	Time 0.025 (0.030)	Data 6.79e-05 (1.71e-03)	Tok/s 25218 (33471)	Loss/tok 3.8124 (3.3529)	LR 5.000e-03
0: TRAIN [4][170/241]	Time 0.025 (0.029)	Data 6.53e-05 (1.61e-03)	Tok/s 26837 (33353)	Loss/tok 2.8876 (3.3405)	LR 5.000e-03
0: TRAIN [4][180/241]	Time 0.025 (0.029)	Data 6.46e-05 (1.53e-03)	Tok/s 26295 (33081)	Loss/tok 2.9935 (3.3248)	LR 5.000e-03
0: TRAIN [4][190/241]	Time 0.025 (0.029)	Data 6.53e-05 (1.45e-03)	Tok/s 24035 (32578)	Loss/tok 2.9777 (3.3116)	LR 5.000e-03
0: TRAIN [4][200/241]	Time 0.025 (0.029)	Data 6.63e-05 (1.38e-03)	Tok/s 28094 (32600)	Loss/tok 3.0618 (3.3033)	LR 5.000e-03
0: TRAIN [4][210/241]	Time 0.025 (0.029)	Data 8.87e-05 (1.32e-03)	Tok/s 26662 (32635)	Loss/tok 3.3399 (3.2986)	LR 5.000e-03
0: TRAIN [4][220/241]	Time 0.028 (0.029)	Data 6.56e-05 (1.27e-03)	Tok/s 38766 (32803)	Loss/tok 2.9332 (3.2981)	LR 5.000e-03
0: TRAIN [4][230/241]	Time 0.028 (0.029)	Data 7.01e-05 (1.21e-03)	Tok/s 39111 (32684)	Loss/tok 3.6217 (3.2927)	LR 5.000e-03
0: TRAIN [4][240/241]	Time 0.025 (0.028)	Data 4.60e-05 (1.17e-03)	Tok/s 26085 (32502)	Loss/tok 3.5237 (3.2856)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880362221, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592880362221, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.259 (0.259)	Decoder iters 110.0 (110.0)	Tok/s 2814 (2814)
0: Running moses detokenizer
0: BLEU(score=22.037404703657774, counts=[35878, 17328, 9623, 5556], totals=[65866, 62863, 59860, 56861], precisions=[54.47119910120548, 27.56470419801791, 16.07584363514868, 9.771196426373086], bp=1.0, sys_len=65866, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880362660, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22039999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592880362661, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.3154	Test BLEU: 22.04
0: Performance: Epoch: 4	Training: 33165308 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1592880362661, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592880362661, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880362662, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 2852581726
0: TRAIN [5][0/241]	Time 0.353 (0.353)	Data 2.81e-01 (2.81e-01)	Tok/s 918 (918)	Loss/tok 2.3856 (2.3856)	LR 5.000e-03
0: TRAIN [5][10/241]	Time 0.022 (0.056)	Data 8.03e-05 (2.56e-02)	Tok/s 14436 (24867)	Loss/tok 2.5656 (3.3391)	LR 5.000e-03
0: TRAIN [5][20/241]	Time 0.025 (0.042)	Data 8.73e-05 (1.35e-02)	Tok/s 25357 (29237)	Loss/tok 2.8245 (3.3681)	LR 5.000e-03
0: TRAIN [5][30/241]	Time 0.025 (0.037)	Data 6.63e-05 (9.13e-03)	Tok/s 25498 (30357)	Loss/tok 2.9024 (3.4049)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [5][40/241]	Time 0.025 (0.035)	Data 6.44e-05 (6.92e-03)	Tok/s 26809 (30422)	Loss/tok 3.5223 (3.3818)	LR 5.000e-03
0: TRAIN [5][50/241]	Time 0.025 (0.033)	Data 6.56e-05 (5.58e-03)	Tok/s 26198 (30751)	Loss/tok 3.1870 (3.3780)	LR 5.000e-03
0: TRAIN [5][60/241]	Time 0.032 (0.032)	Data 8.94e-05 (4.68e-03)	Tok/s 45066 (31213)	Loss/tok 4.2227 (3.3706)	LR 5.000e-03
0: TRAIN [5][70/241]	Time 0.028 (0.032)	Data 6.79e-05 (4.03e-03)	Tok/s 36675 (31567)	Loss/tok 3.5575 (3.3525)	LR 5.000e-03
0: TRAIN [5][80/241]	Time 0.025 (0.031)	Data 6.37e-05 (3.54e-03)	Tok/s 26128 (31650)	Loss/tok 2.8257 (3.3256)	LR 5.000e-03
0: TRAIN [5][90/241]	Time 0.028 (0.030)	Data 6.94e-05 (3.16e-03)	Tok/s 36230 (31621)	Loss/tok 3.2117 (3.3248)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][100/241]	Time 0.025 (0.030)	Data 6.77e-05 (2.85e-03)	Tok/s 28091 (32056)	Loss/tok 2.6006 (3.3112)	LR 5.000e-03
0: TRAIN [5][110/241]	Time 0.025 (0.030)	Data 8.94e-05 (2.60e-03)	Tok/s 26274 (32440)	Loss/tok 3.0571 (3.3151)	LR 5.000e-03
0: TRAIN [5][120/241]	Time 0.025 (0.030)	Data 6.60e-05 (2.39e-03)	Tok/s 26469 (32250)	Loss/tok 3.2309 (3.3113)	LR 5.000e-03
0: TRAIN [5][130/241]	Time 0.032 (0.030)	Data 6.75e-05 (2.22e-03)	Tok/s 44942 (32289)	Loss/tok 3.8450 (3.3120)	LR 5.000e-03
0: TRAIN [5][140/241]	Time 0.025 (0.029)	Data 7.44e-05 (2.07e-03)	Tok/s 26147 (32093)	Loss/tok 3.3839 (3.3134)	LR 5.000e-03
0: TRAIN [5][150/241]	Time 0.032 (0.029)	Data 7.44e-05 (1.93e-03)	Tok/s 45215 (32300)	Loss/tok 4.1404 (3.3303)	LR 5.000e-03
0: TRAIN [5][160/241]	Time 0.025 (0.029)	Data 6.77e-05 (1.82e-03)	Tok/s 27332 (32382)	Loss/tok 3.0491 (3.3343)	LR 5.000e-03
0: TRAIN [5][170/241]	Time 0.028 (0.029)	Data 6.41e-05 (1.72e-03)	Tok/s 36792 (32415)	Loss/tok 3.4099 (3.3283)	LR 5.000e-03
0: TRAIN [5][180/241]	Time 0.025 (0.029)	Data 6.75e-05 (1.62e-03)	Tok/s 26626 (32249)	Loss/tok 2.7702 (3.3227)	LR 5.000e-03
0: TRAIN [5][190/241]	Time 0.025 (0.029)	Data 6.70e-05 (1.54e-03)	Tok/s 25053 (32525)	Loss/tok 3.0856 (3.3288)	LR 5.000e-03
0: TRAIN [5][200/241]	Time 0.028 (0.029)	Data 7.61e-05 (1.47e-03)	Tok/s 38773 (32532)	Loss/tok 3.5749 (3.3306)	LR 5.000e-03
0: TRAIN [5][210/241]	Time 0.028 (0.029)	Data 6.75e-05 (1.40e-03)	Tok/s 37313 (32635)	Loss/tok 3.1479 (3.3234)	LR 5.000e-03
0: TRAIN [5][220/241]	Time 0.025 (0.029)	Data 6.84e-05 (1.34e-03)	Tok/s 25258 (32577)	Loss/tok 2.9013 (3.3163)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [5][230/241]	Time 0.025 (0.029)	Data 7.39e-05 (1.29e-03)	Tok/s 25436 (32514)	Loss/tok 2.8518 (3.3094)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][240/241]	Time 0.025 (0.028)	Data 4.46e-05 (1.24e-03)	Tok/s 25611 (32379)	Loss/tok 3.0977 (3.3054)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880369562, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592880369562, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.338 (0.338)	Decoder iters 149.0 (149.0)	Tok/s 2138 (2138)
0: Running moses detokenizer
0: BLEU(score=22.580763500938193, counts=[35629, 17336, 9683, 5636], totals=[63562, 60559, 57557, 54558], precisions=[56.053931594348825, 28.62662857709011, 16.82332296679813, 10.330290699805712], bp=0.9826264955287712, sys_len=63562, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880370011, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22579999999999997, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592880370011, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.2504	Test BLEU: 22.58
0: Performance: Epoch: 5	Training: 33213944 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1592880370012, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592880370012, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 7, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880370012, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 7}}
0: Starting epoch 6
0: Sampler for epoch 6 uses seed 2761031187
0: TRAIN [6][0/241]	Time 0.354 (0.354)	Data 2.71e-01 (2.71e-01)	Tok/s 1792 (1792)	Loss/tok 3.3903 (3.3903)	LR 5.000e-03
0: TRAIN [6][10/241]	Time 0.025 (0.056)	Data 6.91e-05 (2.47e-02)	Tok/s 26070 (25221)	Loss/tok 2.9135 (3.2088)	LR 5.000e-03
0: TRAIN [6][20/241]	Time 0.036 (0.042)	Data 6.65e-05 (1.30e-02)	Tok/s 52920 (29771)	Loss/tok 3.2151 (3.1839)	LR 5.000e-03
0: TRAIN [6][30/241]	Time 0.025 (0.037)	Data 6.89e-05 (8.82e-03)	Tok/s 27955 (29196)	Loss/tok 2.9744 (3.1396)	LR 5.000e-03
0: TRAIN [6][40/241]	Time 0.025 (0.035)	Data 6.63e-05 (6.69e-03)	Tok/s 24559 (29956)	Loss/tok 3.0823 (3.1374)	LR 5.000e-03
0: TRAIN [6][50/241]	Time 0.032 (0.033)	Data 6.87e-05 (5.39e-03)	Tok/s 45360 (31016)	Loss/tok 3.5527 (3.2063)	LR 5.000e-03
0: TRAIN [6][60/241]	Time 0.022 (0.032)	Data 7.15e-05 (4.52e-03)	Tok/s 15078 (31426)	Loss/tok 2.6536 (3.2034)	LR 5.000e-03
0: TRAIN [6][70/241]	Time 0.022 (0.032)	Data 6.72e-05 (3.89e-03)	Tok/s 17435 (31589)	Loss/tok 2.3629 (3.1978)	LR 5.000e-03
0: TRAIN [6][80/241]	Time 0.025 (0.031)	Data 7.13e-05 (3.42e-03)	Tok/s 26467 (32031)	Loss/tok 3.0164 (3.2123)	LR 5.000e-03
0: TRAIN [6][90/241]	Time 0.025 (0.031)	Data 9.61e-05 (3.05e-03)	Tok/s 25978 (32082)	Loss/tok 2.8650 (3.2148)	LR 5.000e-03
0: TRAIN [6][100/241]	Time 0.025 (0.030)	Data 6.70e-05 (2.76e-03)	Tok/s 26636 (32285)	Loss/tok 3.2218 (3.2119)	LR 5.000e-03
0: TRAIN [6][110/241]	Time 0.025 (0.030)	Data 6.72e-05 (2.52e-03)	Tok/s 26548 (32506)	Loss/tok 2.9724 (3.2323)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [6][120/241]	Time 0.025 (0.030)	Data 7.92e-05 (2.31e-03)	Tok/s 25550 (32613)	Loss/tok 2.8194 (3.2383)	LR 5.000e-03
0: TRAIN [6][130/241]	Time 0.025 (0.030)	Data 6.70e-05 (2.14e-03)	Tok/s 26418 (32533)	Loss/tok 3.2596 (3.2321)	LR 5.000e-03
0: TRAIN [6][140/241]	Time 0.025 (0.029)	Data 7.10e-05 (2.00e-03)	Tok/s 25717 (32339)	Loss/tok 2.9607 (3.2201)	LR 5.000e-03
0: TRAIN [6][150/241]	Time 0.032 (0.029)	Data 6.82e-05 (1.87e-03)	Tok/s 46126 (32142)	Loss/tok 3.3690 (3.2124)	LR 5.000e-03
0: TRAIN [6][160/241]	Time 0.025 (0.029)	Data 6.84e-05 (1.76e-03)	Tok/s 26128 (32249)	Loss/tok 3.2384 (3.2133)	LR 5.000e-03
0: TRAIN [6][170/241]	Time 0.025 (0.029)	Data 6.79e-05 (1.66e-03)	Tok/s 24339 (32389)	Loss/tok 2.6934 (3.2150)	LR 5.000e-03
0: TRAIN [6][180/241]	Time 0.032 (0.029)	Data 7.20e-05 (1.57e-03)	Tok/s 45607 (32415)	Loss/tok 3.0271 (3.2117)	LR 2.500e-03
0: TRAIN [6][190/241]	Time 0.032 (0.029)	Data 7.08e-05 (1.49e-03)	Tok/s 47924 (32371)	Loss/tok 2.9902 (3.2002)	LR 2.500e-03
0: TRAIN [6][200/241]	Time 0.022 (0.029)	Data 6.94e-05 (1.42e-03)	Tok/s 15918 (32204)	Loss/tok 2.8431 (3.1951)	LR 2.500e-03
0: TRAIN [6][210/241]	Time 0.025 (0.029)	Data 8.87e-05 (1.36e-03)	Tok/s 27088 (32264)	Loss/tok 3.0923 (3.2013)	LR 2.500e-03
0: TRAIN [6][220/241]	Time 0.032 (0.029)	Data 6.58e-05 (1.30e-03)	Tok/s 45342 (32535)	Loss/tok 3.5515 (3.2150)	LR 2.500e-03
0: TRAIN [6][230/241]	Time 0.025 (0.028)	Data 6.77e-05 (1.25e-03)	Tok/s 24007 (32298)	Loss/tok 2.8965 (3.2097)	LR 2.500e-03
0: TRAIN [6][240/241]	Time 0.028 (0.028)	Data 4.72e-05 (1.20e-03)	Tok/s 36528 (32320)	Loss/tok 3.0448 (3.2072)	LR 2.500e-03
:::MLLOG {"namespace": "", "time_ms": 1592880376911, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592880376912, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 7}}
0: Running evaluation on test set
0: TEST [6][0/1]	Time 0.276 (0.276)	Decoder iters 105.0 (105.0)	Tok/s 2633 (2633)
0: Running moses detokenizer
0: BLEU(score=23.684840079459644, counts=[36742, 18194, 10306, 6075], totals=[64986, 61983, 58980, 55982], precisions=[56.53833133290247, 29.353209751060774, 17.473719905052562, 10.851702332892716], bp=1.0, sys_len=64986, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880377345, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2368, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592880377345, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 7}}
0: Summary: Epoch: 6	Training Loss: 3.1950	Test BLEU: 23.68
0: Performance: Epoch: 6	Training: 33152624 Tok/s
0: Finished epoch 6
:::MLLOG {"namespace": "", "time_ms": 1592880377346, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592880377346, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 8, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880377346, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 8}}
0: Starting epoch 7
0: Sampler for epoch 7 uses seed 2544549190
0: TRAIN [7][0/241]	Time 0.367 (0.367)	Data 3.27e-01 (3.27e-01)	Tok/s 1629 (1629)	Loss/tok 2.3815 (2.3815)	LR 2.500e-03
0: Upscaling, new scale: 64.0
0: TRAIN [7][10/241]	Time 0.028 (0.057)	Data 1.10e-04 (2.98e-02)	Tok/s 37001 (26603)	Loss/tok 3.0423 (3.0596)	LR 2.500e-03
0: TRAIN [7][20/241]	Time 0.025 (0.042)	Data 6.79e-05 (1.57e-02)	Tok/s 26886 (27696)	Loss/tok 2.7682 (3.0888)	LR 2.500e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [7][30/241]	Time 0.025 (0.038)	Data 8.61e-05 (1.06e-02)	Tok/s 26223 (30088)	Loss/tok 3.2097 (3.1573)	LR 2.500e-03
0: TRAIN [7][40/241]	Time 0.028 (0.035)	Data 7.06e-05 (8.05e-03)	Tok/s 37125 (30616)	Loss/tok 2.8786 (3.1325)	LR 2.500e-03
0: TRAIN [7][50/241]	Time 0.022 (0.033)	Data 9.25e-05 (6.49e-03)	Tok/s 15836 (31007)	Loss/tok 2.1770 (3.0894)	LR 2.500e-03
0: TRAIN [7][60/241]	Time 0.025 (0.032)	Data 1.02e-04 (5.44e-03)	Tok/s 27275 (30800)	Loss/tok 3.2056 (3.0956)	LR 2.500e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [7][70/241]	Time 0.022 (0.032)	Data 7.01e-05 (4.68e-03)	Tok/s 14409 (31539)	Loss/tok 2.7026 (3.1028)	LR 2.500e-03
0: TRAIN [7][80/241]	Time 0.028 (0.031)	Data 6.77e-05 (4.12e-03)	Tok/s 36366 (31925)	Loss/tok 3.0479 (3.0902)	LR 2.500e-03
0: TRAIN [7][90/241]	Time 0.025 (0.031)	Data 6.96e-05 (3.67e-03)	Tok/s 26387 (31738)	Loss/tok 2.7419 (3.0844)	LR 2.500e-03
0: TRAIN [7][100/241]	Time 0.025 (0.030)	Data 6.68e-05 (3.32e-03)	Tok/s 26702 (31230)	Loss/tok 2.8728 (3.0634)	LR 2.500e-03
0: TRAIN [7][110/241]	Time 0.028 (0.030)	Data 8.94e-05 (3.02e-03)	Tok/s 39265 (31812)	Loss/tok 2.8191 (3.0909)	LR 2.500e-03
0: TRAIN [7][120/241]	Time 0.028 (0.030)	Data 7.06e-05 (2.78e-03)	Tok/s 37322 (31694)	Loss/tok 3.0188 (3.0872)	LR 2.500e-03
0: TRAIN [7][130/241]	Time 0.028 (0.029)	Data 6.84e-05 (2.57e-03)	Tok/s 38618 (31701)	Loss/tok 3.2063 (3.0905)	LR 2.500e-03
0: TRAIN [7][140/241]	Time 0.025 (0.029)	Data 9.49e-05 (2.40e-03)	Tok/s 24673 (31789)	Loss/tok 3.4472 (3.0922)	LR 1.250e-03
0: TRAIN [7][150/241]	Time 0.028 (0.029)	Data 9.85e-05 (2.24e-03)	Tok/s 35788 (32125)	Loss/tok 2.9140 (3.0986)	LR 1.250e-03
0: TRAIN [7][160/241]	Time 0.032 (0.029)	Data 6.79e-05 (2.11e-03)	Tok/s 44118 (32269)	Loss/tok 3.6211 (3.1073)	LR 1.250e-03
0: TRAIN [7][170/241]	Time 0.028 (0.029)	Data 6.89e-05 (1.99e-03)	Tok/s 36863 (32302)	Loss/tok 2.9346 (3.1011)	LR 1.250e-03
0: TRAIN [7][180/241]	Time 0.032 (0.029)	Data 9.73e-05 (1.88e-03)	Tok/s 47356 (32480)	Loss/tok 3.2176 (3.1154)	LR 1.250e-03
0: TRAIN [7][190/241]	Time 0.022 (0.029)	Data 7.34e-05 (1.79e-03)	Tok/s 15115 (32402)	Loss/tok 2.5010 (3.1157)	LR 1.250e-03
0: Upscaling, new scale: 32.0
0: TRAIN [7][200/241]	Time 0.022 (0.029)	Data 8.87e-05 (1.70e-03)	Tok/s 16256 (32297)	Loss/tok 2.6831 (3.1093)	LR 1.250e-03
0: TRAIN [7][210/241]	Time 0.025 (0.029)	Data 7.61e-05 (1.63e-03)	Tok/s 24914 (32364)	Loss/tok 2.4399 (3.1082)	LR 1.250e-03
0: TRAIN [7][220/241]	Time 0.028 (0.029)	Data 8.39e-05 (1.56e-03)	Tok/s 37661 (32331)	Loss/tok 3.1816 (3.1042)	LR 1.250e-03
0: TRAIN [7][230/241]	Time 0.028 (0.028)	Data 9.54e-05 (1.49e-03)	Tok/s 37786 (32342)	Loss/tok 3.3864 (3.1067)	LR 1.250e-03
0: TRAIN [7][240/241]	Time 0.025 (0.028)	Data 4.72e-05 (1.43e-03)	Tok/s 26047 (32502)	Loss/tok 2.4749 (3.1063)	LR 1.250e-03
:::MLLOG {"namespace": "", "time_ms": 1592880384250, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592880384251, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 8}}
0: Running evaluation on test set
0: TEST [7][0/1]	Time 0.261 (0.261)	Decoder iters 99.0 (99.0)	Tok/s 2795 (2795)
0: Running moses detokenizer
0: BLEU(score=24.208620113913852, counts=[36734, 18439, 10545, 6252], totals=[64235, 61232, 58229, 55230], precisions=[57.186891881373086, 30.113339430363208, 18.109533050541827, 11.319934818033678], bp=0.9931580970784245, sys_len=64235, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880384685, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2421, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592880384685, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 8}}
0: Summary: Epoch: 7	Training Loss: 3.0920	Test BLEU: 24.21
0: Performance: Epoch: 7	Training: 33188370 Tok/s
0: Finished epoch 7
:::MLLOG {"namespace": "", "time_ms": 1592880384686, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 8}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592880384686, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:31 PM
RESULT,RNN_TRANSLATOR,,100,nvidia,2020-06-22 07:44:51 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/23126/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
slurmstepd: error: _is_a_lwp: open() /proc/96881/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/90581/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/8434/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/29011/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/60588/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/60591/status failed: No such file or directory
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/57835/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/98010/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
slurmstepd: error: _is_a_lwp: open() /proc/76807/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:34 PM
RESULT,RNN_TRANSLATOR,,103,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:35 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:35 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:35 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:35 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:35 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:35 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:44:51 PM
ENDING TIMING RUN AT 2020-06-22 07:46:35 PM
RESULT,RNN_TRANSLATOR,,104,nvidia,2020-06-22 07:44:51 PM
