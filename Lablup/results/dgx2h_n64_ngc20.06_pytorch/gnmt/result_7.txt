+ echo 'Beginning trial 3 of 5'
Beginning trial 3 of 5
+ srun --ntasks=64 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592881176343, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592881176375, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592881176375, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592881176375, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592881176375, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "64xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=64 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n013
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n015
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n052
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n035
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n039
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n029
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n005
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n023
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n003
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n033
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n056
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n007
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n025
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n021
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n094
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n031
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n058
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n054
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n096
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n017
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n009
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n048
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n053
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n057
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n044
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n020
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n059
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n022
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n032
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n024
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n006
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n034
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n051
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n010
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n012
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n042
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n049
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n097
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n004
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n040
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n016
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n008
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n018
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n036
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n055
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n030
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n011
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n027
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n095
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n093
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n026
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n014
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n047
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n043
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n028
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n019
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n002
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n060
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n001
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=64 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592881183171, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183177, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183198, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183206, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183210, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183211, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183214, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183221, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183222, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183231, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183235, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183240, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183239, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183243, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183244, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183244, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183251, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183256, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183279, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183337, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183349, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183363, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183364, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183369, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183371, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183372, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183376, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183377, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183377, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183377, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183381, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183393, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183403, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183409, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183409, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183409, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183413, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183418, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183420, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183420, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183420, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183421, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183431, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183435, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183434, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183450, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183452, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183454, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183456, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183459, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183458, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183462, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183478, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183480, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183482, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183489, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183490, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183510, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183519, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183536, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183559, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881183603, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=1024 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/14043861/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 1 ']'
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 15 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 4 ']'
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
running benchmark
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 14 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
running benchmark
+ DECAY_INTERVAL=201
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ '[' -n 3 ']'
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
running benchmark
+ DIST_OPTS=
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ NUMEPOCHS=14
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ '[' -n 7 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 15 ']'
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 2 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 8 ']'
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 13 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ '[' -n 1 ']'
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 2 ']'
+ '[' -n 3 ']'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
running benchmark
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 5 ']'
running benchmark
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
running benchmark
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
running benchmark
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ WARMUP_STEPS=200
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
running benchmark
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
running benchmark
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
running benchmark
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ '[' -n 6 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ DIST_OPTS=
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ '[' -n 12 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ '[' -n 7 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DECAY_INTERVAL=201
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
running benchmark
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' -n 13 ']'
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 12 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ TARGET=24.0
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ '[' -n 4 ']'
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ DATASET_DIR=/data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
running benchmark
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ declare -a CMD
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 0 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ '[' -n 3 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 12 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ '[' -n 5 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
running benchmark
+ declare -a CMD
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 14 ']'
+ MATH=fp16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 11 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ '[' -n 11 ']'
running benchmark
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 10 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 0 ']'
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ declare -a CMD
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 9 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 13 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ '[' -n 10 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 5 ']'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 2 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ '[' -n 11 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ declare -a CMD
+ '[' -n 12 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 0 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ '[' -n 14 ']'
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ LR=5.0e-3
+ MATH=fp16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ '[' -n 5 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ '[' -n 10 ']'
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
running benchmark
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ '[' -n 10 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 13 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
running benchmark
+ '[' -n 6 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 10 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ LR=5.0e-3
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' -n 12 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 10 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 6 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 7 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ WARMUP_STEPS=200
+ TARGET=24.0
+ '[' -n 5 ']'
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
running benchmark
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 8 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
running benchmark
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 12 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 9 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 12 ']'
+ DATASET_DIR=/data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ declare -a CMD
+ '[' -n 15 ']'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ '[' -n 8 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 6 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 15 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 8 ']'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
running benchmark
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 11 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ '[' -n 5 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 8 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ '[' -n 12 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 2 ']'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 8 ']'
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 7 ']'
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
running benchmark
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ '[' -n 9 ']'
running benchmark
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' -n 5 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 11 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 8 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
running benchmark
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 11 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ '[' -n 7 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
running benchmark
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 10 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ '[' -n 7 ']'
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 14 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 4 ']'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 8 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ '[' -n 13 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 14 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 14 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 15 ']'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 13 ']'
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 1 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 12 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ '[' -n 9 ']'
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 1 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
running benchmark
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 0 ']'
+ '[' -n 10 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 8 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 1 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 13 ']'
+ declare -a CMD
running benchmark
+ '[' -n 4 ']'
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 10 ']'
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ MATH=fp16
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ '[' -n 11 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
running benchmark
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
+ '[' -n 1 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 3 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ MATH=fp16
+ '[' -n 13 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 9 ']'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ '[' -n 7 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 1 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ '[' -n 10 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 8 ']'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ '[' -n 13 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 15 ']'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ '[' -n 7 ']'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ echo 'running benchmark'
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 2 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 12 ']'
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 1 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ '[' -n 0 ']'
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 12 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 9 ']'
running benchmark
+ declare -a CMD
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 10 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 15 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TARGET=24.0
+ '[' -n 2 ']'
+ '[' -n 7 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 14 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ '[' -n 8 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ '[' -n 12 ']'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 13 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 7 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 2 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 6 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 3 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 3 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 13 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 10 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 3 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ '[' -n 7 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 2 ']'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 15 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
running benchmark
+ declare -a CMD
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 0 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
running benchmark
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 15 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 3 ']'
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' -n 3 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 1 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 2 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 12 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
running benchmark
+ '[' -n 11 ']'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 0 ']'
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 3 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 7 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 13 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
num_sockets = 2 num_nodes=2 cores_per_socket=24
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 7 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 10 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
running benchmark
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ declare -a CMD
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 9 ']'
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ '[' -n 15 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 6 ']'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ '[' -n 6 ']'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 3 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 8 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:46 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 11 ']'
+ DIST_OPTS=
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 14 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 4 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:59:47 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188262, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188272, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188285, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188288, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188313, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188311, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188324, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188337, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188337, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188337, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188353, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592881188357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188362, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188362, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188362, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188363, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188364, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188367, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188367, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188366, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188371, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188372, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188385, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188387, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188396, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188412, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188413, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188421, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188421, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188423, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188427, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188432, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188432, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188434, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188441, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188441, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188449, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188465, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188468, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188469, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188478, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188480, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188485, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188493, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188493, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188492, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188498, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188505, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188515, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188517, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188516, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188517, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188516, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188517, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188523, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188525, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188525, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188525, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188528, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188528, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188533, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188534, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188533, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188534, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188537, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188539, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188547, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188551, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188554, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188557, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188556, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188557, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188562, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188563, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188563, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188563, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188565, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188583, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188583, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188588, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188592, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188594, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188594, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188594, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188593, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188596, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188596, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188612, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188615, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188623, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188627, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188627, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188629, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188640, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188682, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188861, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188902, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188918, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188924, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188923, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188925, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188926, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188932, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188936, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188939, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188945, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188950, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188953, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188957, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188958, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188967, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188975, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188981, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188987, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188987, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188998, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188998, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881188998, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189004, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189008, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189008, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189013, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189013, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189015, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189019, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189019, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189019, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189021, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189021, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189022, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189024, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189024, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189026, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189027, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189026, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189027, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189034, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189038, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189038, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189038, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189038, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189048, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189047, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189048, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189048, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189049, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189050, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189052, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189055, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189055, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189061, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189064, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189068, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189071, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189072, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189078, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189085, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189085, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189086, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189088, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189100, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189100, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189100, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189103, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189109, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189117, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189120, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189122, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189122, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189126, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189127, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189127, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189126, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189127, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189128, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189129, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189129, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189129, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189129, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189130, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189130, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189131, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189131, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189131, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189136, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189136, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189136, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189136, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189136, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189142, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189142, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189142, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189144, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189144, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189145, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189145, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189145, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189147, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189147, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189153, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189153, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189153, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189154, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189153, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189154, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189157, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189157, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189159, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189159, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189164, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189162, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189164, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189165, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189165, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189169, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189169, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189166, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189167, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189169, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189170, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189172, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189177, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189177, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189177, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189178, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189180, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189181, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189184, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189187, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189187, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189190, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189192, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189195, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189193, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189197, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189195, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189198, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189200, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189200, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189201, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189203, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189203, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189204, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189214, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189214, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189213, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189214, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189215, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189220, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189220, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189227, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189216, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189226, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189227, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189234, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189234, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189237, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189236, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189237, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189237, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189237, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189237, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189239, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189240, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189239, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189241, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189240, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189239, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189241, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189241, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189244, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189244, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189244, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189245, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189246, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189246, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189247, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189248, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189247, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189247, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189248, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189249, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189249, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189249, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189251, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189250, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189251, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189250, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189250, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189250, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189251, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189251, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189255, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189255, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189258, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189260, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189259, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189262, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189260, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189262, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189269, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189264, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189265, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189267, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189265, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189269, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189268, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189270, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189271, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189271, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189270, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189272, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189272, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189273, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189274, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189275, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189277, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189276, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189277, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189280, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189279, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189282, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189285, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189290, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189292, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189292, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189294, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189296, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189297, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189302, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189312, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189309, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189310, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189313, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189312, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189314, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189319, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189330, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189336, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189338, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189343, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189345, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189351, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189351, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189361, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189368, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189369, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189373, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189386, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189389, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189390, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189391, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189391, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189395, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189396, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189398, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189404, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189406, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592881189420, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=32134259, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=201, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=True, env=False, epochs=14, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.005, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=1605, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=4, test_loader_workers=0, train_batch_size=16, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3856654943
:::MLLOG {"namespace": "", "time_ms": 1592881212990, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3856654943, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3854029631
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.005}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.005
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592881216439, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592881216440, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.005, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592881216440, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592881216440, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592881216440, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592881219700, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592881220857, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592881220858, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592881221109, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16384, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592881221110, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3948544, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592881221110, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 1605, 'decay_interval': 201, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1605
0: Scheduler decay interval: 201
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592881221110, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592881221110, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592881221111, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 201, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592881221111, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592881221111, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592881221111, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 1605, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592881221111, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592881221111, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881221111, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 4124917844
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/241]	Time 0.456 (0.456)	Data 2.72e-01 (2.72e-01)	Tok/s 1492 (1492)	Loss/tok 10.7862 (10.7862)	LR 5.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/241]	Time 0.042 (0.091)	Data 6.84e-05 (2.48e-02)	Tok/s 43645 (29756)	Loss/tok 9.9999 (10.5455)	LR 5.610e-05
0: TRAIN [0][20/241]	Time 0.028 (0.061)	Data 1.18e-04 (1.30e-02)	Tok/s 40417 (30197)	Loss/tok 9.1836 (10.0010)	LR 7.063e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: TRAIN [0][30/241]	Time 0.025 (0.049)	Data 7.89e-05 (8.86e-03)	Tok/s 27021 (29365)	Loss/tok 8.5440 (9.7195)	LR 8.491e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: TRAIN [0][40/241]	Time 0.028 (0.044)	Data 6.65e-05 (6.71e-03)	Tok/s 39491 (30763)	Loss/tok 9.0416 (9.5030)	LR 1.045e-04
0: TRAIN [0][50/241]	Time 0.025 (0.041)	Data 6.87e-05 (5.41e-03)	Tok/s 25171 (31138)	Loss/tok 8.1856 (9.2929)	LR 1.315e-04
0: TRAIN [0][60/241]	Time 0.025 (0.038)	Data 6.68e-05 (4.53e-03)	Tok/s 27334 (31411)	Loss/tok 7.9248 (9.1075)	LR 1.656e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 1.0
0: TRAIN [0][70/241]	Time 0.025 (0.037)	Data 6.56e-05 (3.90e-03)	Tok/s 25427 (31061)	Loss/tok 7.8918 (8.9709)	LR 2.037e-04
0: TRAIN [0][80/241]	Time 0.028 (0.036)	Data 8.56e-05 (3.43e-03)	Tok/s 39111 (31416)	Loss/tok 7.9179 (8.8675)	LR 2.564e-04
0: TRAIN [0][90/241]	Time 0.032 (0.035)	Data 6.32e-05 (3.06e-03)	Tok/s 46046 (32169)	Loss/tok 7.9250 (8.7405)	LR 3.228e-04
0: TRAIN [0][100/241]	Time 0.025 (0.034)	Data 6.68e-05 (2.76e-03)	Tok/s 26590 (32407)	Loss/tok 7.5024 (8.6476)	LR 4.064e-04
0: TRAIN [0][110/241]	Time 0.025 (0.033)	Data 6.37e-05 (2.52e-03)	Tok/s 24903 (32632)	Loss/tok 7.7027 (8.5675)	LR 5.116e-04
0: TRAIN [0][120/241]	Time 0.025 (0.033)	Data 6.51e-05 (2.32e-03)	Tok/s 26395 (32662)	Loss/tok 7.6200 (8.4995)	LR 6.441e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 0.5
0: TRAIN [0][130/241]	Time 0.025 (0.032)	Data 6.68e-05 (2.15e-03)	Tok/s 25563 (32500)	Loss/tok 7.4750 (8.4603)	LR 7.924e-04
0: TRAIN [0][140/241]	Time 0.022 (0.032)	Data 6.48e-05 (2.00e-03)	Tok/s 16526 (31980)	Loss/tok 6.6036 (8.4060)	LR 9.976e-04
0: TRAIN [0][150/241]	Time 0.025 (0.032)	Data 6.29e-05 (1.87e-03)	Tok/s 25554 (32006)	Loss/tok 7.1734 (8.3471)	LR 1.256e-03
0: TRAIN [0][160/241]	Time 0.025 (0.031)	Data 6.34e-05 (1.76e-03)	Tok/s 24297 (31880)	Loss/tok 7.1933 (8.2815)	LR 1.581e-03
0: TRAIN [0][170/241]	Time 0.025 (0.031)	Data 8.61e-05 (1.66e-03)	Tok/s 23593 (32112)	Loss/tok 6.1850 (8.1966)	LR 1.991e-03
0: TRAIN [0][180/241]	Time 0.025 (0.031)	Data 6.46e-05 (1.57e-03)	Tok/s 25676 (31923)	Loss/tok 6.3805 (8.1265)	LR 2.506e-03
0: TRAIN [0][190/241]	Time 0.028 (0.031)	Data 6.68e-05 (1.49e-03)	Tok/s 37974 (31851)	Loss/tok 6.4607 (8.0526)	LR 3.155e-03
0: TRAIN [0][200/241]	Time 0.025 (0.030)	Data 6.25e-05 (1.42e-03)	Tok/s 24995 (31951)	Loss/tok 6.2913 (7.9706)	LR 3.972e-03
0: TRAIN [0][210/241]	Time 0.028 (0.030)	Data 6.08e-05 (1.36e-03)	Tok/s 38772 (32076)	Loss/tok 6.1872 (7.8855)	LR 5.000e-03
0: TRAIN [0][220/241]	Time 0.028 (0.030)	Data 6.56e-05 (1.30e-03)	Tok/s 36831 (32360)	Loss/tok 6.1542 (7.7979)	LR 5.000e-03
0: TRAIN [0][230/241]	Time 0.025 (0.030)	Data 6.39e-05 (1.25e-03)	Tok/s 27009 (32240)	Loss/tok 5.4503 (7.7294)	LR 5.000e-03
0: TRAIN [0][240/241]	Time 0.028 (0.030)	Data 4.27e-05 (1.20e-03)	Tok/s 38069 (32255)	Loss/tok 5.5849 (7.6487)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881228353, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881228353, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.338 (0.338)	Decoder iters 149.0 (149.0)	Tok/s 1848 (1848)
0: Running moses detokenizer
0: BLEU(score=0.4752332594922045, counts=[13984, 1325, 173, 39], totals=[129671, 126668, 123665, 120663], precisions=[10.784215437530365, 1.0460416206145198, 0.13989406865321635, 0.03232142413167251], bp=1.0, sys_len=129671, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881228864, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.0048, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881228864, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 7.6493	Test BLEU: 0.48
0: Performance: Epoch: 0	Training: 33171248 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592881228865, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881228865, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881228865, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 3827780816
0: TRAIN [1][0/241]	Time 0.357 (0.357)	Data 2.72e-01 (2.72e-01)	Tok/s 1837 (1837)	Loss/tok 5.4657 (5.4657)	LR 5.000e-03
0: Upscaling, new scale: 1.0
0: TRAIN [1][10/241]	Time 0.028 (0.058)	Data 6.79e-05 (2.48e-02)	Tok/s 37646 (33210)	Loss/tok 5.8180 (5.7061)	LR 5.000e-03
0: TRAIN [1][20/241]	Time 0.025 (0.043)	Data 6.84e-05 (1.30e-02)	Tok/s 28657 (34091)	Loss/tok 5.3926 (5.5790)	LR 5.000e-03
0: TRAIN [1][30/241]	Time 0.025 (0.038)	Data 6.58e-05 (8.86e-03)	Tok/s 24783 (33991)	Loss/tok 4.7494 (5.4659)	LR 5.000e-03
0: TRAIN [1][40/241]	Time 0.028 (0.035)	Data 6.79e-05 (6.71e-03)	Tok/s 37321 (33634)	Loss/tok 4.6178 (5.3438)	LR 5.000e-03
0: TRAIN [1][50/241]	Time 0.025 (0.034)	Data 6.60e-05 (5.41e-03)	Tok/s 23181 (32956)	Loss/tok 4.4728 (5.2333)	LR 5.000e-03
0: TRAIN [1][60/241]	Time 0.025 (0.033)	Data 6.39e-05 (4.53e-03)	Tok/s 26889 (33473)	Loss/tok 4.6078 (5.1757)	LR 5.000e-03
0: TRAIN [1][70/241]	Time 0.025 (0.032)	Data 6.75e-05 (3.91e-03)	Tok/s 24980 (33637)	Loss/tok 4.7207 (5.1115)	LR 5.000e-03
0: TRAIN [1][80/241]	Time 0.025 (0.031)	Data 6.65e-05 (3.43e-03)	Tok/s 28032 (33168)	Loss/tok 4.0966 (5.0484)	LR 5.000e-03
0: TRAIN [1][90/241]	Time 0.028 (0.031)	Data 6.48e-05 (3.06e-03)	Tok/s 37223 (32630)	Loss/tok 4.3224 (4.9761)	LR 5.000e-03
0: TRAIN [1][100/241]	Time 0.036 (0.030)	Data 6.87e-05 (2.77e-03)	Tok/s 53658 (32715)	Loss/tok 4.5523 (4.9041)	LR 5.000e-03
0: TRAIN [1][110/241]	Time 0.036 (0.030)	Data 6.89e-05 (2.52e-03)	Tok/s 53383 (33038)	Loss/tok 4.9181 (4.8794)	LR 5.000e-03
0: TRAIN [1][120/241]	Time 0.025 (0.030)	Data 6.94e-05 (2.32e-03)	Tok/s 25386 (32915)	Loss/tok 4.0953 (4.8246)	LR 5.000e-03
0: TRAIN [1][130/241]	Time 0.025 (0.030)	Data 6.63e-05 (2.15e-03)	Tok/s 26685 (32817)	Loss/tok 3.6622 (4.7809)	LR 5.000e-03
0: Upscaling, new scale: 2.0
0: TRAIN [1][140/241]	Time 0.032 (0.029)	Data 6.87e-05 (2.00e-03)	Tok/s 45621 (32738)	Loss/tok 4.5437 (4.7360)	LR 5.000e-03
0: TRAIN [1][150/241]	Time 0.026 (0.029)	Data 7.03e-05 (1.87e-03)	Tok/s 23089 (32384)	Loss/tok 3.4784 (4.6848)	LR 5.000e-03
0: TRAIN [1][160/241]	Time 0.029 (0.029)	Data 6.91e-05 (1.76e-03)	Tok/s 36766 (32701)	Loss/tok 4.2411 (4.6363)	LR 5.000e-03
0: TRAIN [1][170/241]	Time 0.025 (0.029)	Data 6.75e-05 (1.66e-03)	Tok/s 24916 (32718)	Loss/tok 3.1883 (4.5996)	LR 5.000e-03
0: TRAIN [1][180/241]	Time 0.029 (0.029)	Data 6.53e-05 (1.57e-03)	Tok/s 37086 (32857)	Loss/tok 4.2023 (4.5801)	LR 5.000e-03
0: TRAIN [1][190/241]	Time 0.025 (0.029)	Data 6.79e-05 (1.50e-03)	Tok/s 28313 (32863)	Loss/tok 3.0631 (4.5303)	LR 5.000e-03
0: TRAIN [1][200/241]	Time 0.032 (0.029)	Data 6.53e-05 (1.42e-03)	Tok/s 46918 (32574)	Loss/tok 4.0968 (4.4975)	LR 5.000e-03
0: TRAIN [1][210/241]	Time 0.028 (0.029)	Data 6.79e-05 (1.36e-03)	Tok/s 37601 (32626)	Loss/tok 3.6391 (4.4714)	LR 5.000e-03
0: TRAIN [1][220/241]	Time 0.025 (0.029)	Data 6.65e-05 (1.30e-03)	Tok/s 28116 (32571)	Loss/tok 3.7080 (4.4460)	LR 5.000e-03
0: TRAIN [1][230/241]	Time 0.036 (0.029)	Data 6.72e-05 (1.25e-03)	Tok/s 53344 (32584)	Loss/tok 4.2064 (4.4217)	LR 5.000e-03
0: TRAIN [1][240/241]	Time 0.025 (0.028)	Data 4.48e-05 (1.20e-03)	Tok/s 24642 (32412)	Loss/tok 3.6531 (4.3914)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881235760, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881235760, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.263 (0.263)	Decoder iters 108.0 (108.0)	Tok/s 2346 (2346)
0: Running moses detokenizer
0: BLEU(score=14.617888579488369, counts=[27944, 11741, 5909, 3091], totals=[53911, 50908, 47906, 44909], precisions=[51.83357756302053, 23.063172782273906, 12.33457186991191, 6.88280745507582], bp=0.8189920061730847, sys_len=53911, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881236253, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1462, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881236253, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 4.3941	Test BLEU: 14.62
0: Performance: Epoch: 1	Training: 33186490 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592881236254, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592881236254, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881236254, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3569277847
0: TRAIN [2][0/241]	Time 0.369 (0.369)	Data 2.82e-01 (2.82e-01)	Tok/s 4958 (4958)	Loss/tok 4.3051 (4.3051)	LR 5.000e-03
0: TRAIN [2][10/241]	Time 0.022 (0.057)	Data 6.89e-05 (2.57e-02)	Tok/s 15541 (27676)	Loss/tok 2.6022 (3.7405)	LR 5.000e-03
0: TRAIN [2][20/241]	Time 0.025 (0.042)	Data 6.75e-05 (1.35e-02)	Tok/s 24946 (28806)	Loss/tok 3.7094 (3.7268)	LR 5.000e-03
0: Upscaling, new scale: 4.0
0: TRAIN [2][30/241]	Time 0.028 (0.037)	Data 6.48e-05 (9.18e-03)	Tok/s 37712 (30215)	Loss/tok 3.8166 (3.7124)	LR 5.000e-03
0: TRAIN [2][40/241]	Time 0.025 (0.035)	Data 6.94e-05 (6.95e-03)	Tok/s 27172 (30851)	Loss/tok 3.4675 (3.7314)	LR 5.000e-03
0: TRAIN [2][50/241]	Time 0.025 (0.034)	Data 6.79e-05 (5.60e-03)	Tok/s 25826 (31840)	Loss/tok 3.2952 (3.7359)	LR 5.000e-03
0: TRAIN [2][60/241]	Time 0.025 (0.033)	Data 6.82e-05 (4.70e-03)	Tok/s 27520 (32644)	Loss/tok 3.4304 (3.7540)	LR 5.000e-03
0: TRAIN [2][70/241]	Time 0.028 (0.032)	Data 1.07e-04 (4.05e-03)	Tok/s 39900 (32939)	Loss/tok 3.4373 (3.7376)	LR 5.000e-03
0: TRAIN [2][80/241]	Time 0.032 (0.031)	Data 6.48e-05 (3.56e-03)	Tok/s 46928 (32889)	Loss/tok 4.1410 (3.7264)	LR 5.000e-03
0: TRAIN [2][90/241]	Time 0.025 (0.031)	Data 6.94e-05 (3.17e-03)	Tok/s 24265 (33114)	Loss/tok 3.6453 (3.7020)	LR 5.000e-03
0: TRAIN [2][100/241]	Time 0.028 (0.030)	Data 6.89e-05 (2.87e-03)	Tok/s 37610 (33070)	Loss/tok 4.1863 (3.6863)	LR 5.000e-03
0: TRAIN [2][110/241]	Time 0.022 (0.030)	Data 6.39e-05 (2.61e-03)	Tok/s 16125 (32995)	Loss/tok 3.3512 (3.6624)	LR 5.000e-03
0: TRAIN [2][120/241]	Time 0.028 (0.030)	Data 6.53e-05 (2.40e-03)	Tok/s 37307 (32839)	Loss/tok 3.5047 (3.6621)	LR 5.000e-03
0: TRAIN [2][130/241]	Time 0.032 (0.029)	Data 6.48e-05 (2.22e-03)	Tok/s 47253 (32563)	Loss/tok 3.8403 (3.6578)	LR 5.000e-03
0: TRAIN [2][140/241]	Time 0.025 (0.029)	Data 6.68e-05 (2.07e-03)	Tok/s 25642 (32159)	Loss/tok 3.5390 (3.6419)	LR 5.000e-03
0: TRAIN [2][150/241]	Time 0.032 (0.029)	Data 6.60e-05 (1.94e-03)	Tok/s 44949 (32009)	Loss/tok 3.9649 (3.6331)	LR 5.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [2][160/241]	Time 0.025 (0.029)	Data 6.89e-05 (1.82e-03)	Tok/s 24033 (32369)	Loss/tok 3.5713 (3.6480)	LR 5.000e-03
0: TRAIN [2][170/241]	Time 0.028 (0.029)	Data 6.68e-05 (1.72e-03)	Tok/s 36092 (32385)	Loss/tok 3.8015 (3.6566)	LR 5.000e-03
0: TRAIN [2][180/241]	Time 0.028 (0.029)	Data 6.68e-05 (1.63e-03)	Tok/s 38927 (32318)	Loss/tok 3.7310 (3.6466)	LR 5.000e-03
0: TRAIN [2][190/241]	Time 0.028 (0.029)	Data 7.96e-05 (1.55e-03)	Tok/s 39197 (32436)	Loss/tok 3.2952 (3.6377)	LR 5.000e-03
0: TRAIN [2][200/241]	Time 0.028 (0.029)	Data 6.58e-05 (1.47e-03)	Tok/s 38608 (32410)	Loss/tok 2.9597 (3.6326)	LR 5.000e-03
0: TRAIN [2][210/241]	Time 0.025 (0.029)	Data 6.56e-05 (1.41e-03)	Tok/s 27427 (32553)	Loss/tok 3.0759 (3.6335)	LR 5.000e-03
0: TRAIN [2][220/241]	Time 0.025 (0.028)	Data 6.48e-05 (1.35e-03)	Tok/s 26298 (32584)	Loss/tok 3.6572 (3.6285)	LR 5.000e-03
0: TRAIN [2][230/241]	Time 0.036 (0.028)	Data 6.37e-05 (1.29e-03)	Tok/s 53987 (32723)	Loss/tok 3.5184 (3.6220)	LR 5.000e-03
0: TRAIN [2][240/241]	Time 0.028 (0.028)	Data 4.55e-05 (1.24e-03)	Tok/s 36362 (32678)	Loss/tok 3.7583 (3.6171)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881243127, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881243127, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.329 (0.329)	Decoder iters 149.0 (149.0)	Tok/s 2437 (2437)
0: Running moses detokenizer
0: BLEU(score=18.57813564212452, counts=[34932, 15908, 8437, 4663], totals=[70041, 67038, 64035, 61037], precisions=[49.87364543624449, 23.72982487544378, 13.175607089872726, 7.639628422104625], bp=1.0, sys_len=70041, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881243623, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1858, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881243623, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.6166	Test BLEU: 18.58
0: Performance: Epoch: 2	Training: 33209344 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592881243624, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592881243624, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881243624, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 2198461866
0: TRAIN [3][0/241]	Time 0.369 (0.369)	Data 3.12e-01 (3.12e-01)	Tok/s 2835 (2835)	Loss/tok 3.4661 (3.4661)	LR 5.000e-03
0: TRAIN [3][10/241]	Time 0.028 (0.057)	Data 9.58e-05 (2.85e-02)	Tok/s 37871 (28693)	Loss/tok 3.4132 (3.1852)	LR 5.000e-03
0: TRAIN [3][20/241]	Time 0.025 (0.043)	Data 7.27e-05 (1.49e-02)	Tok/s 25291 (30621)	Loss/tok 3.4398 (3.2934)	LR 5.000e-03
0: TRAIN [3][30/241]	Time 0.022 (0.038)	Data 7.32e-05 (1.01e-02)	Tok/s 15750 (30324)	Loss/tok 2.7201 (3.2980)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [3][40/241]	Time 0.025 (0.035)	Data 7.27e-05 (7.69e-03)	Tok/s 25922 (29621)	Loss/tok 3.4207 (3.3279)	LR 5.000e-03
0: TRAIN [3][50/241]	Time 0.025 (0.033)	Data 1.04e-04 (6.20e-03)	Tok/s 25410 (30307)	Loss/tok 2.9651 (3.3242)	LR 5.000e-03
0: TRAIN [3][60/241]	Time 0.025 (0.032)	Data 6.99e-05 (5.19e-03)	Tok/s 24090 (31139)	Loss/tok 3.2754 (3.3601)	LR 5.000e-03
0: TRAIN [3][70/241]	Time 0.028 (0.031)	Data 8.37e-05 (4.47e-03)	Tok/s 37024 (31037)	Loss/tok 3.7619 (3.3400)	LR 5.000e-03
0: TRAIN [3][80/241]	Time 0.025 (0.031)	Data 6.68e-05 (3.93e-03)	Tok/s 26808 (31416)	Loss/tok 3.1419 (3.3468)	LR 5.000e-03
0: TRAIN [3][90/241]	Time 0.028 (0.031)	Data 7.30e-05 (3.51e-03)	Tok/s 36950 (31955)	Loss/tok 3.5682 (3.3504)	LR 5.000e-03
0: TRAIN [3][100/241]	Time 0.028 (0.030)	Data 7.08e-05 (3.17e-03)	Tok/s 38767 (31797)	Loss/tok 3.4540 (3.3563)	LR 5.000e-03
0: TRAIN [3][110/241]	Time 0.025 (0.030)	Data 7.01e-05 (2.89e-03)	Tok/s 27179 (31816)	Loss/tok 2.9581 (3.3699)	LR 5.000e-03
0: TRAIN [3][120/241]	Time 0.022 (0.029)	Data 6.91e-05 (2.66e-03)	Tok/s 14489 (31583)	Loss/tok 2.8135 (3.3615)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [3][130/241]	Time 0.028 (0.029)	Data 7.32e-05 (2.46e-03)	Tok/s 37400 (32043)	Loss/tok 3.1837 (3.3795)	LR 5.000e-03
0: TRAIN [3][140/241]	Time 0.025 (0.029)	Data 6.65e-05 (2.29e-03)	Tok/s 27659 (32025)	Loss/tok 3.3897 (3.3820)	LR 5.000e-03
0: TRAIN [3][150/241]	Time 0.025 (0.029)	Data 6.99e-05 (2.14e-03)	Tok/s 24749 (31932)	Loss/tok 3.6037 (3.3837)	LR 5.000e-03
0: TRAIN [3][160/241]	Time 0.032 (0.029)	Data 6.84e-05 (2.01e-03)	Tok/s 45449 (31938)	Loss/tok 3.5669 (3.3951)	LR 5.000e-03
0: TRAIN [3][170/241]	Time 0.025 (0.029)	Data 6.27e-05 (1.90e-03)	Tok/s 27555 (31975)	Loss/tok 3.0995 (3.3929)	LR 5.000e-03
0: TRAIN [3][180/241]	Time 0.025 (0.029)	Data 6.53e-05 (1.80e-03)	Tok/s 25171 (32039)	Loss/tok 3.0931 (3.3831)	LR 5.000e-03
0: TRAIN [3][190/241]	Time 0.025 (0.029)	Data 6.89e-05 (1.71e-03)	Tok/s 26980 (32033)	Loss/tok 3.4339 (3.3948)	LR 5.000e-03
0: TRAIN [3][200/241]	Time 0.028 (0.029)	Data 6.84e-05 (1.63e-03)	Tok/s 34731 (32182)	Loss/tok 3.2843 (3.4071)	LR 5.000e-03
0: TRAIN [3][210/241]	Time 0.025 (0.029)	Data 6.75e-05 (1.55e-03)	Tok/s 22843 (32273)	Loss/tok 2.9264 (3.4073)	LR 5.000e-03
0: TRAIN [3][220/241]	Time 0.028 (0.028)	Data 6.82e-05 (1.48e-03)	Tok/s 36942 (32328)	Loss/tok 3.1468 (3.4091)	LR 5.000e-03
0: TRAIN [3][230/241]	Time 0.028 (0.028)	Data 6.75e-05 (1.42e-03)	Tok/s 38770 (32560)	Loss/tok 3.6811 (3.4073)	LR 5.000e-03
0: TRAIN [3][240/241]	Time 0.036 (0.028)	Data 4.22e-05 (1.37e-03)	Tok/s 50894 (32567)	Loss/tok 3.6919 (3.4072)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881250512, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881250512, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.248 (0.248)	Decoder iters 106.0 (106.0)	Tok/s 2792 (2792)
0: Running moses detokenizer
0: BLEU(score=21.289382311999752, counts=[35012, 16527, 9033, 5105], totals=[64002, 60999, 57997, 55001], precisions=[54.7045404831099, 27.093886785029262, 15.574943531561978, 9.281649424555917], bp=0.9895243351032074, sys_len=64002, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881250950, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21289999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881250950, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.4109	Test BLEU: 21.29
0: Performance: Epoch: 3	Training: 33224880 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592881250950, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592881250950, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881250950, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 2781280849
0: TRAIN [4][0/241]	Time 0.359 (0.359)	Data 3.15e-01 (3.15e-01)	Tok/s 3085 (3085)	Loss/tok 3.0343 (3.0343)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [4][10/241]	Time 0.028 (0.059)	Data 6.75e-05 (2.87e-02)	Tok/s 37104 (33872)	Loss/tok 3.6826 (3.5369)	LR 5.000e-03
0: TRAIN [4][20/241]	Time 0.025 (0.043)	Data 6.53e-05 (1.51e-02)	Tok/s 24987 (31128)	Loss/tok 2.8312 (3.3238)	LR 5.000e-03
0: TRAIN [4][30/241]	Time 0.028 (0.038)	Data 6.53e-05 (1.02e-02)	Tok/s 39534 (32061)	Loss/tok 3.0749 (3.3131)	LR 5.000e-03
0: TRAIN [4][40/241]	Time 0.025 (0.035)	Data 7.68e-05 (7.76e-03)	Tok/s 27111 (31414)	Loss/tok 2.8299 (3.3284)	LR 5.000e-03
0: TRAIN [4][50/241]	Time 0.028 (0.033)	Data 6.53e-05 (6.25e-03)	Tok/s 36343 (31289)	Loss/tok 3.5511 (3.3168)	LR 5.000e-03
0: TRAIN [4][60/241]	Time 0.028 (0.032)	Data 6.87e-05 (5.24e-03)	Tok/s 37005 (31533)	Loss/tok 3.0998 (3.3132)	LR 5.000e-03
0: TRAIN [4][70/241]	Time 0.025 (0.032)	Data 6.53e-05 (4.51e-03)	Tok/s 27244 (32110)	Loss/tok 3.3372 (3.3404)	LR 5.000e-03
0: TRAIN [4][80/241]	Time 0.025 (0.031)	Data 7.06e-05 (3.96e-03)	Tok/s 25417 (32554)	Loss/tok 3.1158 (3.3356)	LR 5.000e-03
0: TRAIN [4][90/241]	Time 0.025 (0.031)	Data 6.72e-05 (3.53e-03)	Tok/s 25399 (32782)	Loss/tok 3.3263 (3.3628)	LR 5.000e-03
0: TRAIN [4][100/241]	Time 0.025 (0.030)	Data 7.10e-05 (3.19e-03)	Tok/s 26326 (33002)	Loss/tok 3.1677 (3.3601)	LR 5.000e-03
0: TRAIN [4][110/241]	Time 0.022 (0.030)	Data 7.22e-05 (2.91e-03)	Tok/s 15573 (33270)	Loss/tok 2.8854 (3.3624)	LR 5.000e-03
0: TRAIN [4][120/241]	Time 0.025 (0.030)	Data 6.91e-05 (2.68e-03)	Tok/s 25243 (33120)	Loss/tok 2.8566 (3.3495)	LR 5.000e-03
0: TRAIN [4][130/241]	Time 0.028 (0.030)	Data 8.44e-05 (2.48e-03)	Tok/s 40430 (33066)	Loss/tok 2.8591 (3.3456)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [4][140/241]	Time 0.036 (0.029)	Data 6.94e-05 (2.31e-03)	Tok/s 52040 (32895)	Loss/tok 3.8312 (3.3497)	LR 5.000e-03
0: TRAIN [4][150/241]	Time 0.028 (0.029)	Data 6.84e-05 (2.16e-03)	Tok/s 37506 (32757)	Loss/tok 3.2685 (3.3393)	LR 5.000e-03
0: TRAIN [4][160/241]	Time 0.025 (0.029)	Data 7.01e-05 (2.03e-03)	Tok/s 25240 (32683)	Loss/tok 3.0963 (3.3331)	LR 5.000e-03
0: TRAIN [4][170/241]	Time 0.028 (0.029)	Data 6.84e-05 (1.91e-03)	Tok/s 36260 (32432)	Loss/tok 3.3366 (3.3255)	LR 5.000e-03
0: TRAIN [4][180/241]	Time 0.028 (0.029)	Data 7.13e-05 (1.81e-03)	Tok/s 38329 (32410)	Loss/tok 3.3650 (3.3328)	LR 5.000e-03
0: TRAIN [4][190/241]	Time 0.028 (0.029)	Data 9.63e-05 (1.72e-03)	Tok/s 36343 (32403)	Loss/tok 3.6646 (3.3345)	LR 5.000e-03
0: TRAIN [4][200/241]	Time 0.028 (0.029)	Data 6.87e-05 (1.64e-03)	Tok/s 36803 (32517)	Loss/tok 3.1745 (3.3334)	LR 5.000e-03
0: TRAIN [4][210/241]	Time 0.022 (0.029)	Data 6.82e-05 (1.56e-03)	Tok/s 14647 (32552)	Loss/tok 2.8548 (3.3308)	LR 5.000e-03
0: TRAIN [4][220/241]	Time 0.028 (0.028)	Data 6.99e-05 (1.50e-03)	Tok/s 37834 (32438)	Loss/tok 3.4066 (3.3302)	LR 5.000e-03
0: TRAIN [4][230/241]	Time 0.025 (0.028)	Data 9.23e-05 (1.44e-03)	Tok/s 24226 (32566)	Loss/tok 2.7529 (3.3268)	LR 5.000e-03
0: TRAIN [4][240/241]	Time 0.025 (0.028)	Data 4.46e-05 (1.38e-03)	Tok/s 28098 (32480)	Loss/tok 2.7973 (3.3250)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881257822, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881257822, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.341 (0.341)	Decoder iters 149.0 (149.0)	Tok/s 2494 (2494)
0: Running moses detokenizer
0: BLEU(score=21.86585428306575, counts=[35589, 17009, 9368, 5359], totals=[64979, 61976, 58973, 55977], precisions=[54.77000261622986, 27.444494643087648, 15.885235616298985, 9.573574861103667], bp=1.0, sys_len=64979, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881258267, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2187, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881258268, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.3113	Test BLEU: 21.87
0: Performance: Epoch: 4	Training: 33256394 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1592881258268, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592881258268, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881258268, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 3903687316
0: TRAIN [5][0/241]	Time 0.354 (0.354)	Data 2.79e-01 (2.79e-01)	Tok/s 988 (988)	Loss/tok 2.8285 (2.8285)	LR 5.000e-03
0: TRAIN [5][10/241]	Time 0.028 (0.056)	Data 7.03e-05 (2.54e-02)	Tok/s 37494 (28579)	Loss/tok 3.1057 (3.1369)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][20/241]	Time 0.032 (0.042)	Data 7.15e-05 (1.34e-02)	Tok/s 43070 (29310)	Loss/tok 4.0007 (3.1120)	LR 5.000e-03
0: TRAIN [5][30/241]	Time 0.022 (0.037)	Data 6.77e-05 (9.07e-03)	Tok/s 16211 (30398)	Loss/tok 2.7497 (3.1774)	LR 5.000e-03
0: TRAIN [5][40/241]	Time 0.036 (0.034)	Data 6.51e-05 (6.87e-03)	Tok/s 53809 (30132)	Loss/tok 3.4966 (3.2323)	LR 5.000e-03
0: TRAIN [5][50/241]	Time 0.025 (0.033)	Data 6.58e-05 (5.54e-03)	Tok/s 23313 (30527)	Loss/tok 3.0251 (3.2554)	LR 5.000e-03
0: TRAIN [5][60/241]	Time 0.028 (0.032)	Data 6.58e-05 (4.64e-03)	Tok/s 38173 (31168)	Loss/tok 2.9754 (3.2380)	LR 5.000e-03
0: TRAIN [5][70/241]	Time 0.025 (0.031)	Data 6.84e-05 (4.00e-03)	Tok/s 28462 (31561)	Loss/tok 3.0105 (3.2231)	LR 5.000e-03
0: TRAIN [5][80/241]	Time 0.028 (0.031)	Data 7.13e-05 (3.52e-03)	Tok/s 37073 (31526)	Loss/tok 2.9819 (3.2197)	LR 5.000e-03
0: TRAIN [5][90/241]	Time 0.032 (0.030)	Data 6.34e-05 (3.14e-03)	Tok/s 45402 (31980)	Loss/tok 3.5203 (3.2428)	LR 5.000e-03
0: TRAIN [5][100/241]	Time 0.025 (0.030)	Data 6.63e-05 (2.83e-03)	Tok/s 26290 (31853)	Loss/tok 3.0506 (3.2253)	LR 5.000e-03
0: TRAIN [5][110/241]	Time 0.022 (0.030)	Data 6.79e-05 (2.58e-03)	Tok/s 14701 (31853)	Loss/tok 2.6706 (3.2273)	LR 5.000e-03
0: TRAIN [5][120/241]	Time 0.028 (0.029)	Data 6.89e-05 (2.38e-03)	Tok/s 37036 (31806)	Loss/tok 3.2199 (3.2249)	LR 5.000e-03
0: TRAIN [5][130/241]	Time 0.025 (0.029)	Data 6.89e-05 (2.20e-03)	Tok/s 28309 (31854)	Loss/tok 3.4996 (3.2029)	LR 5.000e-03
0: TRAIN [5][140/241]	Time 0.028 (0.029)	Data 6.75e-05 (2.05e-03)	Tok/s 36435 (31835)	Loss/tok 2.9441 (3.1931)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [5][150/241]	Time 0.032 (0.029)	Data 7.15e-05 (1.92e-03)	Tok/s 45559 (32602)	Loss/tok 3.6043 (3.2228)	LR 5.000e-03
0: TRAIN [5][160/241]	Time 0.028 (0.029)	Data 6.77e-05 (1.80e-03)	Tok/s 38111 (32882)	Loss/tok 2.9509 (3.2346)	LR 5.000e-03
0: TRAIN [5][170/241]	Time 0.028 (0.029)	Data 6.96e-05 (1.70e-03)	Tok/s 37633 (33038)	Loss/tok 3.6773 (3.2385)	LR 5.000e-03
0: TRAIN [5][180/241]	Time 0.025 (0.029)	Data 6.75e-05 (1.61e-03)	Tok/s 24493 (32968)	Loss/tok 2.9855 (3.2390)	LR 5.000e-03
0: TRAIN [5][190/241]	Time 0.028 (0.029)	Data 7.77e-05 (1.53e-03)	Tok/s 36175 (33018)	Loss/tok 3.3739 (3.2424)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][200/241]	Time 0.022 (0.029)	Data 6.75e-05 (1.46e-03)	Tok/s 15461 (32985)	Loss/tok 2.2973 (3.2466)	LR 5.000e-03
0: TRAIN [5][210/241]	Time 0.025 (0.029)	Data 6.87e-05 (1.39e-03)	Tok/s 25095 (32714)	Loss/tok 2.7621 (3.2428)	LR 5.000e-03
0: TRAIN [5][220/241]	Time 0.025 (0.029)	Data 6.68e-05 (1.33e-03)	Tok/s 27677 (32947)	Loss/tok 3.0558 (3.2503)	LR 5.000e-03
0: TRAIN [5][230/241]	Time 0.025 (0.028)	Data 6.84e-05 (1.28e-03)	Tok/s 26778 (32677)	Loss/tok 3.0797 (3.2472)	LR 5.000e-03
0: TRAIN [5][240/241]	Time 0.036 (0.028)	Data 4.53e-05 (1.23e-03)	Tok/s 54045 (32598)	Loss/tok 3.4095 (3.2405)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592881265143, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881265144, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.342 (0.342)	Decoder iters 149.0 (149.0)	Tok/s 2250 (2250)
0: Running moses detokenizer
0: BLEU(score=22.61325081813968, counts=[36453, 17784, 9929, 5810], totals=[66091, 63088, 60085, 57087], precisions=[55.15577007459412, 28.189196043621607, 16.524923025713573, 10.177448455865608], bp=1.0, sys_len=66091, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881265590, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2261, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881265591, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.2471	Test BLEU: 22.61
0: Performance: Epoch: 5	Training: 33330504 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1592881265591, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592881265591, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 7, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881265591, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 7}}
0: Starting epoch 6
0: Sampler for epoch 6 uses seed 3390786422
0: TRAIN [6][0/241]	Time 0.365 (0.365)	Data 2.91e-01 (2.91e-01)	Tok/s 1734 (1734)	Loss/tok 3.5188 (3.5188)	LR 5.000e-03
0: TRAIN [6][10/241]	Time 0.028 (0.057)	Data 7.08e-05 (2.66e-02)	Tok/s 38063 (28447)	Loss/tok 2.9627 (3.0893)	LR 5.000e-03
0: TRAIN [6][20/241]	Time 0.028 (0.043)	Data 6.99e-05 (1.40e-02)	Tok/s 37509 (31083)	Loss/tok 3.1343 (3.1707)	LR 5.000e-03
0: TRAIN [6][30/241]	Time 0.028 (0.037)	Data 6.94e-05 (9.48e-03)	Tok/s 37294 (30664)	Loss/tok 3.2800 (3.1631)	LR 5.000e-03
0: TRAIN [6][40/241]	Time 0.028 (0.035)	Data 7.41e-05 (7.18e-03)	Tok/s 36256 (31058)	Loss/tok 3.4094 (3.2171)	LR 5.000e-03
0: TRAIN [6][50/241]	Time 0.032 (0.033)	Data 6.82e-05 (5.79e-03)	Tok/s 46908 (31133)	Loss/tok 3.2920 (3.2189)	LR 5.000e-03
0: TRAIN [6][60/241]	Time 0.028 (0.032)	Data 6.68e-05 (4.85e-03)	Tok/s 35296 (30732)	Loss/tok 3.3855 (3.2133)	LR 5.000e-03
0: TRAIN [6][70/241]	Time 0.025 (0.032)	Data 6.94e-05 (4.18e-03)	Tok/s 24272 (31769)	Loss/tok 3.1628 (3.2335)	LR 5.000e-03
0: TRAIN [6][80/241]	Time 0.028 (0.031)	Data 6.79e-05 (3.67e-03)	Tok/s 39832 (31437)	Loss/tok 3.4348 (3.2279)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [6][90/241]	Time 0.032 (0.030)	Data 6.87e-05 (3.28e-03)	Tok/s 46158 (31754)	Loss/tok 3.2639 (3.2408)	LR 5.000e-03
0: TRAIN [6][100/241]	Time 0.028 (0.030)	Data 6.58e-05 (2.96e-03)	Tok/s 40256 (32116)	Loss/tok 2.9671 (3.2445)	LR 5.000e-03
0: TRAIN [6][110/241]	Time 0.025 (0.030)	Data 6.89e-05 (2.70e-03)	Tok/s 26130 (32200)	Loss/tok 2.9303 (3.2303)	LR 5.000e-03
0: TRAIN [6][120/241]	Time 0.022 (0.030)	Data 6.75e-05 (2.48e-03)	Tok/s 13981 (32018)	Loss/tok 2.5071 (3.2130)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [6][130/241]	Time 0.032 (0.029)	Data 6.84e-05 (2.30e-03)	Tok/s 45505 (32190)	Loss/tok 3.4931 (3.2201)	LR 5.000e-03
0: TRAIN [6][140/241]	Time 0.025 (0.029)	Data 7.03e-05 (2.14e-03)	Tok/s 25401 (32199)	Loss/tok 2.5640 (3.2153)	LR 5.000e-03
0: TRAIN [6][150/241]	Time 0.025 (0.029)	Data 6.99e-05 (2.00e-03)	Tok/s 26484 (32320)	Loss/tok 2.5393 (3.2175)	LR 5.000e-03
0: TRAIN [6][160/241]	Time 0.025 (0.029)	Data 6.94e-05 (1.88e-03)	Tok/s 27803 (32174)	Loss/tok 2.8388 (3.2125)	LR 5.000e-03
0: TRAIN [6][170/241]	Time 0.036 (0.029)	Data 7.03e-05 (1.78e-03)	Tok/s 56553 (32275)	Loss/tok 3.3466 (3.2106)	LR 5.000e-03
0: TRAIN [6][180/241]	Time 0.036 (0.029)	Data 6.99e-05 (1.68e-03)	Tok/s 55151 (32400)	Loss/tok 3.0543 (3.2127)	LR 2.500e-03
0: TRAIN [6][190/241]	Time 0.028 (0.029)	Data 6.72e-05 (1.60e-03)	Tok/s 36909 (32397)	Loss/tok 2.8822 (3.2116)	LR 2.500e-03
0: TRAIN [6][200/241]	Time 0.028 (0.029)	Data 6.99e-05 (1.52e-03)	Tok/s 35005 (32445)	Loss/tok 3.4628 (3.2171)	LR 2.500e-03
0: TRAIN [6][210/241]	Time 0.025 (0.029)	Data 8.03e-05 (1.45e-03)	Tok/s 26633 (32487)	Loss/tok 3.1447 (3.2189)	LR 2.500e-03
0: TRAIN [6][220/241]	Time 0.028 (0.028)	Data 7.22e-05 (1.39e-03)	Tok/s 36848 (32357)	Loss/tok 2.9736 (3.2126)	LR 2.500e-03
0: TRAIN [6][230/241]	Time 0.025 (0.028)	Data 8.56e-05 (1.33e-03)	Tok/s 27693 (32341)	Loss/tok 3.4923 (3.2061)	LR 2.500e-03
0: TRAIN [6][240/241]	Time 0.028 (0.028)	Data 6.48e-05 (1.28e-03)	Tok/s 38780 (32505)	Loss/tok 3.0884 (3.2054)	LR 2.500e-03
:::MLLOG {"namespace": "", "time_ms": 1592881272476, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881272476, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 7}}
0: Running evaluation on test set
0: TEST [6][0/1]	Time 0.311 (0.311)	Decoder iters 121.0 (121.0)	Tok/s 2277 (2277)
0: Running moses detokenizer
0: BLEU(score=23.07567275735158, counts=[36588, 18004, 10130, 5939], totals=[65739, 62736, 59733, 56736], precisions=[55.65645963583261, 28.69803621525121, 16.958799993303533, 10.46778059785674], bp=1.0, sys_len=65739, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881272909, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23079999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881272910, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 7}}
0: Summary: Epoch: 6	Training Loss: 3.1911	Test BLEU: 23.08
0: Performance: Epoch: 6	Training: 33280370 Tok/s
0: Finished epoch 6
:::MLLOG {"namespace": "", "time_ms": 1592881272910, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592881272910, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 8, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881272910, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 8}}
0: Starting epoch 7
0: Sampler for epoch 7 uses seed 2048563818
0: TRAIN [7][0/241]	Time 0.359 (0.359)	Data 3.29e-01 (3.29e-01)	Tok/s 2930 (2930)	Loss/tok 3.1195 (3.1195)	LR 2.500e-03
0: Upscaling, new scale: 32.0
0: TRAIN [7][10/241]	Time 0.028 (0.057)	Data 7.03e-05 (2.99e-02)	Tok/s 36525 (29991)	Loss/tok 3.2610 (3.1155)	LR 2.500e-03
0: TRAIN [7][20/241]	Time 0.028 (0.042)	Data 6.94e-05 (1.57e-02)	Tok/s 37733 (29815)	Loss/tok 3.1922 (3.1198)	LR 2.500e-03
0: TRAIN [7][30/241]	Time 0.028 (0.037)	Data 6.68e-05 (1.07e-02)	Tok/s 35422 (30702)	Loss/tok 3.1033 (3.0887)	LR 2.500e-03
0: TRAIN [7][40/241]	Time 0.032 (0.035)	Data 6.75e-05 (8.08e-03)	Tok/s 45011 (30769)	Loss/tok 3.3287 (3.0610)	LR 2.500e-03
0: TRAIN [7][50/241]	Time 0.028 (0.033)	Data 6.63e-05 (6.51e-03)	Tok/s 37894 (31527)	Loss/tok 2.7594 (3.0710)	LR 2.500e-03
0: TRAIN [7][60/241]	Time 0.028 (0.033)	Data 6.75e-05 (5.45e-03)	Tok/s 37850 (32514)	Loss/tok 3.0663 (3.0836)	LR 2.500e-03
0: TRAIN [7][70/241]	Time 0.028 (0.032)	Data 6.68e-05 (4.70e-03)	Tok/s 36967 (32645)	Loss/tok 3.4131 (3.0942)	LR 2.500e-03
0: TRAIN [7][80/241]	Time 0.028 (0.031)	Data 6.82e-05 (4.12e-03)	Tok/s 37028 (32428)	Loss/tok 2.8604 (3.0972)	LR 2.500e-03
0: TRAIN [7][90/241]	Time 0.028 (0.031)	Data 6.70e-05 (3.68e-03)	Tok/s 36350 (32327)	Loss/tok 3.2286 (3.0938)	LR 2.500e-03
0: TRAIN [7][100/241]	Time 0.032 (0.030)	Data 6.60e-05 (3.32e-03)	Tok/s 46488 (32112)	Loss/tok 3.2685 (3.0924)	LR 2.500e-03
0: TRAIN [7][110/241]	Time 0.025 (0.030)	Data 6.65e-05 (3.03e-03)	Tok/s 25606 (31757)	Loss/tok 3.3220 (3.0866)	LR 2.500e-03
0: TRAIN [7][120/241]	Time 0.028 (0.030)	Data 6.53e-05 (2.78e-03)	Tok/s 37054 (31965)	Loss/tok 2.6977 (3.0974)	LR 2.500e-03
0: TRAIN [7][130/241]	Time 0.032 (0.029)	Data 6.58e-05 (2.58e-03)	Tok/s 44982 (32075)	Loss/tok 3.8229 (3.0877)	LR 2.500e-03
0: Upscaling, new scale: 64.0
0: TRAIN [7][140/241]	Time 0.025 (0.029)	Data 6.63e-05 (2.40e-03)	Tok/s 26288 (32397)	Loss/tok 3.0029 (3.1060)	LR 1.250e-03
0: TRAIN [7][150/241]	Time 0.025 (0.029)	Data 6.68e-05 (2.24e-03)	Tok/s 28966 (32616)	Loss/tok 2.5301 (3.1024)	LR 1.250e-03
0: TRAIN [7][160/241]	Time 0.032 (0.029)	Data 6.94e-05 (2.11e-03)	Tok/s 45385 (32742)	Loss/tok 3.5020 (3.1101)	LR 1.250e-03
0: TRAIN [7][170/241]	Time 0.025 (0.029)	Data 6.56e-05 (1.99e-03)	Tok/s 27791 (32716)	Loss/tok 3.1160 (3.1101)	LR 1.250e-03
0: TRAIN [7][180/241]	Time 0.025 (0.029)	Data 6.65e-05 (1.88e-03)	Tok/s 25772 (33032)	Loss/tok 3.1281 (3.1184)	LR 1.250e-03
0: TRAIN [7][190/241]	Time 0.028 (0.029)	Data 6.79e-05 (1.79e-03)	Tok/s 37735 (33262)	Loss/tok 3.4844 (3.1170)	LR 1.250e-03
0: TRAIN [7][200/241]	Time 0.025 (0.029)	Data 6.84e-05 (1.70e-03)	Tok/s 28611 (33247)	Loss/tok 3.0551 (3.1108)	LR 1.250e-03
0: TRAIN [7][210/241]	Time 0.025 (0.029)	Data 6.41e-05 (1.63e-03)	Tok/s 25697 (33065)	Loss/tok 2.7505 (3.1079)	LR 1.250e-03
0: TRAIN [7][220/241]	Time 0.028 (0.029)	Data 8.32e-05 (1.55e-03)	Tok/s 38598 (32999)	Loss/tok 3.0179 (3.1074)	LR 1.250e-03
0: TRAIN [7][230/241]	Time 0.028 (0.028)	Data 6.68e-05 (1.49e-03)	Tok/s 38907 (32996)	Loss/tok 3.3001 (3.1052)	LR 1.250e-03
0: TRAIN [7][240/241]	Time 0.028 (0.028)	Data 4.53e-05 (1.43e-03)	Tok/s 36604 (32683)	Loss/tok 2.9968 (3.1004)	LR 1.250e-03
:::MLLOG {"namespace": "", "time_ms": 1592881279784, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881279784, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 8}}
0: Running evaluation on test set
0: TEST [7][0/1]	Time 0.248 (0.248)	Decoder iters 107.0 (107.0)	Tok/s 2850 (2850)
0: Running moses detokenizer
0: BLEU(score=23.61396703609868, counts=[36485, 18108, 10212, 6000], totals=[64514, 61511, 58508, 55512], precisions=[56.553616269336885, 29.438636991757573, 17.454023381417926, 10.8084738434933], bp=0.9974920672655461, sys_len=64514, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881280223, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2361, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881280224, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 8}}
0: Summary: Epoch: 7	Training Loss: 3.0933	Test BLEU: 23.61
0: Performance: Epoch: 7	Training: 33244088 Tok/s
0: Finished epoch 7
:::MLLOG {"namespace": "", "time_ms": 1592881280224, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592881280224, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 9, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881280224, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 9}}
0: Starting epoch 8
0: Sampler for epoch 8 uses seed 1453757483
0: TRAIN [8][0/241]	Time 0.352 (0.352)	Data 3.12e-01 (3.12e-01)	Tok/s 2903 (2903)	Loss/tok 3.3478 (3.3478)	LR 1.250e-03
0: TRAIN [8][10/241]	Time 0.028 (0.058)	Data 6.82e-05 (2.84e-02)	Tok/s 38209 (33455)	Loss/tok 2.9330 (3.1425)	LR 1.250e-03
0: TRAIN [8][20/241]	Time 0.022 (0.042)	Data 6.94e-05 (1.49e-02)	Tok/s 14606 (31595)	Loss/tok 2.6403 (3.0812)	LR 1.250e-03
0: Upscaling, new scale: 128.0
0: TRAIN [8][30/241]	Time 0.025 (0.038)	Data 6.70e-05 (1.01e-02)	Tok/s 24748 (32122)	Loss/tok 2.6665 (3.0892)	LR 1.250e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [8][40/241]	Time 0.025 (0.035)	Data 8.08e-05 (7.67e-03)	Tok/s 42590 (32965)	Loss/tok 2.9732 (3.0452)	LR 1.250e-03
0: TRAIN [8][50/241]	Time 0.028 (0.034)	Data 6.65e-05 (6.18e-03)	Tok/s 37194 (32595)	Loss/tok 3.3241 (3.0906)	LR 1.250e-03
0: TRAIN [8][60/241]	Time 0.025 (0.032)	Data 6.87e-05 (5.18e-03)	Tok/s 26088 (32391)	Loss/tok 2.6925 (3.0633)	LR 1.250e-03
0: TRAIN [8][70/241]	Time 0.028 (0.032)	Data 1.10e-04 (4.46e-03)	Tok/s 36851 (32732)	Loss/tok 2.7661 (3.0370)	LR 1.250e-03
0: TRAIN [8][80/241]	Time 0.028 (0.031)	Data 9.23e-05 (3.92e-03)	Tok/s 38019 (32002)	Loss/tok 2.9857 (3.0259)	LR 1.250e-03
0: TRAIN [8][90/241]	Time 0.032 (0.030)	Data 6.68e-05 (3.50e-03)	Tok/s 46832 (31961)	Loss/tok 2.7054 (3.0289)	LR 1.250e-03
0: TRAIN [8][100/241]	Time 0.025 (0.030)	Data 9.63e-05 (3.16e-03)	Tok/s 26124 (31946)	Loss/tok 2.9780 (3.0286)	LR 6.250e-04
0: TRAIN [8][110/241]	Time 0.028 (0.030)	Data 6.91e-05 (2.88e-03)	Tok/s 38291 (31954)	Loss/tok 2.8467 (3.0326)	LR 6.250e-04
0: TRAIN [8][120/241]	Time 0.025 (0.030)	Data 1.24e-04 (2.65e-03)	Tok/s 26618 (32287)	Loss/tok 2.6903 (3.0390)	LR 6.250e-04
0: TRAIN [8][130/241]	Time 0.025 (0.029)	Data 6.72e-05 (2.45e-03)	Tok/s 28277 (32304)	Loss/tok 2.8788 (3.0351)	LR 6.250e-04
0: TRAIN [8][140/241]	Time 0.032 (0.029)	Data 6.77e-05 (2.28e-03)	Tok/s 46048 (32637)	Loss/tok 2.9468 (3.0485)	LR 6.250e-04
0: TRAIN [8][150/241]	Time 0.025 (0.029)	Data 6.99e-05 (2.14e-03)	Tok/s 26085 (32578)	Loss/tok 2.8589 (3.0486)	LR 6.250e-04
0: TRAIN [8][160/241]	Time 0.028 (0.029)	Data 7.41e-05 (2.01e-03)	Tok/s 36203 (32431)	Loss/tok 3.1807 (3.0417)	LR 6.250e-04
0: Upscaling, new scale: 128.0
0: TRAIN [8][170/241]	Time 0.028 (0.029)	Data 6.79e-05 (1.90e-03)	Tok/s 37308 (32524)	Loss/tok 3.1917 (3.0464)	LR 6.250e-04
0: TRAIN [8][180/241]	Time 0.028 (0.029)	Data 6.72e-05 (1.80e-03)	Tok/s 36939 (32422)	Loss/tok 3.1408 (3.0433)	LR 6.250e-04
0: TRAIN [8][190/241]	Time 0.028 (0.029)	Data 6.82e-05 (1.70e-03)	Tok/s 36935 (32190)	Loss/tok 3.2602 (3.0420)	LR 6.250e-04
0: TRAIN [8][200/241]	Time 0.025 (0.029)	Data 6.79e-05 (1.62e-03)	Tok/s 25091 (32187)	Loss/tok 2.8738 (3.0368)	LR 6.250e-04
0: TRAIN [8][210/241]	Time 0.036 (0.028)	Data 6.70e-05 (1.55e-03)	Tok/s 51719 (32298)	Loss/tok 3.5870 (3.0399)	LR 6.250e-04
0: TRAIN [8][220/241]	Time 0.025 (0.028)	Data 6.75e-05 (1.48e-03)	Tok/s 24644 (32338)	Loss/tok 2.6898 (3.0443)	LR 6.250e-04
0: TRAIN [8][230/241]	Time 0.021 (0.028)	Data 7.22e-05 (1.42e-03)	Tok/s 13402 (32234)	Loss/tok 2.3436 (3.0453)	LR 6.250e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [8][240/241]	Time 0.028 (0.028)	Data 4.36e-05 (1.37e-03)	Tok/s 38169 (32403)	Loss/tok 2.9188 (3.0430)	LR 6.250e-04
:::MLLOG {"namespace": "", "time_ms": 1592881287089, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 9}}
:::MLLOG {"namespace": "", "time_ms": 1592881287089, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 9}}
0: Running evaluation on test set
0: TEST [8][0/1]	Time 0.248 (0.248)	Decoder iters 102.0 (102.0)	Tok/s 2808 (2808)
0: Running moses detokenizer
0: BLEU(score=23.832655405913272, counts=[36690, 18254, 10355, 6110], totals=[64797, 61794, 58791, 55795], precisions=[56.62299180517616, 29.540084797876816, 17.61324012178735, 10.950802043193834], bp=1.0, sys_len=64797, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881287524, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23829999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 9}}
:::MLLOG {"namespace": "", "time_ms": 1592881287524, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 9}}
0: Summary: Epoch: 8	Training Loss: 3.0416	Test BLEU: 23.83
0: Performance: Epoch: 8	Training: 33264968 Tok/s
0: Finished epoch 8
:::MLLOG {"namespace": "", "time_ms": 1592881287524, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 9}}
:::MLLOG {"namespace": "", "time_ms": 1592881287525, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 10, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592881287525, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 10}}
0: Starting epoch 9
0: Sampler for epoch 9 uses seed 3291848130
0: TRAIN [9][0/241]	Time 0.357 (0.357)	Data 3.18e-01 (3.18e-01)	Tok/s 2895 (2895)	Loss/tok 3.3115 (3.3115)	LR 6.250e-04
0: TRAIN [9][10/241]	Time 0.028 (0.057)	Data 6.87e-05 (2.90e-02)	Tok/s 38646 (30839)	Loss/tok 2.8177 (3.0154)	LR 6.250e-04
0: TRAIN [9][20/241]	Time 0.028 (0.043)	Data 7.10e-05 (1.52e-02)	Tok/s 36909 (33240)	Loss/tok 2.8429 (3.0444)	LR 6.250e-04
0: TRAIN [9][30/241]	Time 0.022 (0.038)	Data 6.63e-05 (1.03e-02)	Tok/s 16202 (32884)	Loss/tok 2.3180 (3.0493)	LR 6.250e-04
0: TRAIN [9][40/241]	Time 0.032 (0.035)	Data 6.87e-05 (7.83e-03)	Tok/s 45217 (33153)	Loss/tok 3.4076 (3.0613)	LR 6.250e-04
0: TRAIN [9][50/241]	Time 0.032 (0.034)	Data 6.87e-05 (6.31e-03)	Tok/s 46198 (34580)	Loss/tok 3.2054 (3.0712)	LR 6.250e-04
0: TRAIN [9][60/241]	Time 0.025 (0.033)	Data 7.03e-05 (5.28e-03)	Tok/s 26638 (34422)	Loss/tok 3.0385 (3.0429)	LR 3.125e-04
0: TRAIN [9][70/241]	Time 0.028 (0.032)	Data 6.68e-05 (4.55e-03)	Tok/s 38274 (34016)	Loss/tok 2.8638 (3.0371)	LR 3.125e-04
0: TRAIN [9][80/241]	Time 0.025 (0.032)	Data 6.77e-05 (4.00e-03)	Tok/s 25405 (33748)	Loss/tok 3.0956 (3.0319)	LR 3.125e-04
0: TRAIN [9][90/241]	Time 0.028 (0.031)	Data 6.82e-05 (3.56e-03)	Tok/s 37577 (33678)	Loss/tok 3.1002 (3.0273)	LR 3.125e-04
0: TRAIN [9][100/241]	Time 0.025 (0.031)	Data 6.87e-05 (3.22e-03)	Tok/s 25848 (33457)	Loss/tok 2.8567 (3.0233)	LR 3.125e-04
0: TRAIN [9][110/241]	Time 0.025 (0.030)	Data 7.39e-05 (2.94e-03)	Tok/s 24536 (32872)	Loss/tok 2.9887 (3.0114)	LR 3.125e-04
0: Upscaling, new scale: 128.0
0: TRAIN [9][120/241]	Time 0.021 (0.030)	Data 7.63e-05 (2.70e-03)	Tok/s 14411 (33016)	Loss/tok 2.3852 (3.0206)	LR 3.125e-04
0: TRAIN [9][130/241]	Time 0.025 (0.030)	Data 8.08e-05 (2.50e-03)	Tok/s 25654 (33285)	Loss/tok 3.5302 (3.0332)	LR 3.125e-04
0: TRAIN [9][140/241]	Time 0.025 (0.030)	Data 7.10e-05 (2.33e-03)	Tok/s 25710 (33077)	Loss/tok 2.8868 (3.0299)	LR 3.125e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [9][150/241]	Time 0.023 (0.029)	Data 7.72e-05 (2.18e-03)	Tok/s 26217 (33152)	Loss/tok 2.9511 (3.0359)	LR 3.125e-04
0: TRAIN [9][160/241]	Time 0.025 (0.029)	Data 7.10e-05 (2.05e-03)	Tok/s 25155 (32851)	Loss/tok 3.1981 (3.0326)	LR 3.125e-04
0: TRAIN [9][170/241]	Time 0.022 (0.029)	Data 1.02e-04 (1.93e-03)	Tok/s 16743 (32586)	Loss/tok 2.5529 (3.0281)	LR 3.125e-04
0: TRAIN [9][180/241]	Time 0.028 (0.029)	Data 6.89e-05 (1.83e-03)	Tok/s 37289 (32539)	Loss/tok 2.6754 (3.0164)	LR 3.125e-04
0: TRAIN [9][190/241]	Time 0.032 (0.029)	Data 9.01e-05 (1.74e-03)	Tok/s 46985 (32488)	Loss/tok 2.7867 (3.0039)	LR 3.125e-04
0: TRAIN [9][200/241]	Time 0.028 (0.029)	Data 8.63e-05 (1.66e-03)	Tok/s 39045 (32642)	Loss/tok 2.9729 (3.0070)	LR 3.125e-04
0: TRAIN [9][210/241]	Time 0.036 (0.029)	Data 7.15e-05 (1.58e-03)	Tok/s 51826 (32710)	Loss/tok 3.4035 (3.0184)	LR 3.125e-04
0: TRAIN [9][220/241]	Time 0.025 (0.029)	Data 7.03e-05 (1.51e-03)	Tok/s 27178 (32685)	Loss/tok 3.1410 (3.0214)	LR 3.125e-04
0: TRAIN [9][230/241]	Time 0.028 (0.028)	Data 9.42e-05 (1.45e-03)	Tok/s 37734 (32589)	Loss/tok 3.5145 (3.0249)	LR 3.125e-04
0: TRAIN [9][240/241]	Time 0.028 (0.028)	Data 4.51e-05 (1.39e-03)	Tok/s 38075 (32518)	Loss/tok 2.8630 (3.0230)	LR 3.125e-04
:::MLLOG {"namespace": "", "time_ms": 1592881294399, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1592881294400, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 10}}
0: Running evaluation on test set
0: TEST [9][0/1]	Time 0.278 (0.278)	Decoder iters 107.0 (107.0)	Tok/s 2549 (2549)
0: Running moses detokenizer
0: BLEU(score=24.10366723382218, counts=[36682, 18373, 10477, 6234], totals=[64690, 61687, 58684, 55687], precisions=[56.704281960117484, 29.784233306855576, 17.853247904028354, 11.194713308312533], bp=1.0, sys_len=64690, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592881294785, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24100000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 10}}
:::MLLOG {"namespace": "", "time_ms": 1592881294785, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 10}}
0: Summary: Epoch: 9	Training Loss: 3.0174	Test BLEU: 24.10
0: Performance: Epoch: 9	Training: 33224146 Tok/s
0: Finished epoch 9
:::MLLOG {"namespace": "", "time_ms": 1592881294785, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 10}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592881294786, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,114,nvidia,2020-06-22 07:59:47 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:41 PM
RESULT,RNN_TRANSLATOR,,115,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:42 PM
RESULT,RNN_TRANSLATOR,,116,nvidia,2020-06-22 07:59:46 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
slurmstepd: error: _is_a_lwp: open() /proc/44867/status failed: No such file or directory
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
slurmstepd: error: _is_a_lwp: open() /proc/89263/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/12305/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:47 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/23795/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/15483/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/10198/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/85808/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
slurmstepd: error: _is_a_lwp: open() /proc/91128/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,117,nvidia,2020-06-22 07:59:47 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:44 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,118,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
ENDING TIMING RUN AT 2020-06-22 08:01:45 PM
RESULT,RNN_TRANSLATOR,,119,nvidia,2020-06-22 07:59:46 PM
